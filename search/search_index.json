{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Azure TRE Overview Trusted Research Environments (TRE) enforce a secure boundary around distinct workspaces to enable information governance controls to be enforced. A Trusted Research Environment (typically one per organization, or one per department in large organizations) consist of One Composition Service (API, deployment engine etc. used to manage and deploy workspaces, workspace services and user resources) One set of Shared Services used by all workspaces A number of workspaces, where each workspace is its own security boundary, and in turn contains Workspace Services and User Resources Following are more detailed descriptions of the TRE concepts Composition Service and API Services Shared Services Workspace Workspace Service User Resource Templates Application components of the TRE A TRE consist of multiple processes orchestrating managing workspaces and services. These are components that enable researchers and TRE admins to provision and manage workspaces in a self-service manner. The components are of relevance for Azure administrators , TRE service integrator and TRE developers . Composition Service The Composition Service offers an abstraction over the lower-level Azure resources to allow for TRE users to provision resources in terms of workspaces and workspace services. The Composition Service exposes resources \u2013 based on above concepts \u2013 as an HTTP API where users and applications can model the desired representation of the TRE, i.e., define which workspaces should contain which workspace services The Composition Service reconciles the desired state with the actual state by invoking Azure resource deployments. Services A service provide one or more capabilities to you as a user of the TRE or to the TRE itself. Depending on the type of the service it is scoped to the environment and shared across all workspaces (Shared Service) or scoped to a specific workspace (Workspace Service). The types of services required for a research project varies greatly why extensibility is a key aspect of the Azure TRE solution. New services can be developed by you and your organization to fit your needs. Some Workspace Services are accessible from outside the protected network, such as a Virtual Desktop. No data will be permitted to be transferred outside the protected network. Other services such as Azure Machine Learning might need access restricting to via a Virtual Desktop. Shared Services Shared Services are services and resource shared by all workspaces. Firewall Package Mirror Git Mirror Workspace A workspace is a set of resources on a network with inbound traffic, restricted to authorised users, and outbound access restricted to defined network locations. The workspace is a security boundary and there should be zero transfer of data out from the workspace unless explicitly configured. Data transfer is not restricted within a workspace. The workspace itself contains only the bare essentials to provide this functionality, such as firewalls, storage etc. Workspaces can be enhanced with one or more building blocks called workspace services like Azure ML, Guacamole etc. to allow functionality such as development of machine learning models, data engineering, data analysis and software development. Multiple workspaces can be created within a single Trusted Research Environment to create the required separation for your projects. Each workspace has workspace users : one workspace owner, and one or more workspace researchers that can access the data and workspace services in the workspace. The workspace owner is also considered a workspace researcher. Workspace Service A workspace service is a service, created as a building block, with pre-configured set of resources that can be applied to a workspace. Examples of Workspace Services are: Guacamole (Virtual Desktops) Azure Machine Learning Unlike shared services, a workspace service is only accessible to the workspace users. Some workspace services, such as Guacamole, allow users to add on user-specific resources (user resources) All workspace services can be deployed to all workspaces. User Resource A user resource is a resource that is only available to a particular researcher. For example a Guacamole VM. User resources can be deployed to workspaces with a compatible workspace service. E.g. Guacamole VMs can only be deployed to workspaces where the Guacamole workspace service is deployed. Templates In order to deploy resources (workspaces, workspace services, user resources), the resources have to be defined in templates. A template contains everything needed to create an instance of the resource. Ex. a base workspace template, or a Guacamole workspace service template. The templates describe the porter bundles used, and the input parameters needed to deploy them. To use a template, and deploy a resource, the template needs to be registered in the TRE. This is done using the TRE API. Tip Once a template is registered it can be used multiple times to deploy multiple workspaces, workspace services etc. If you want to author your own workspace, workspace service, or user resource template, consult the template authoring guide","title":"Concepts"},{"location":"#azure-tre-overview","text":"Trusted Research Environments (TRE) enforce a secure boundary around distinct workspaces to enable information governance controls to be enforced. A Trusted Research Environment (typically one per organization, or one per department in large organizations) consist of One Composition Service (API, deployment engine etc. used to manage and deploy workspaces, workspace services and user resources) One set of Shared Services used by all workspaces A number of workspaces, where each workspace is its own security boundary, and in turn contains Workspace Services and User Resources Following are more detailed descriptions of the TRE concepts Composition Service and API Services Shared Services Workspace Workspace Service User Resource Templates","title":"Azure TRE Overview"},{"location":"#application-components-of-the-tre","text":"A TRE consist of multiple processes orchestrating managing workspaces and services. These are components that enable researchers and TRE admins to provision and manage workspaces in a self-service manner. The components are of relevance for Azure administrators , TRE service integrator and TRE developers .","title":"Application components of the TRE"},{"location":"#composition-service","text":"The Composition Service offers an abstraction over the lower-level Azure resources to allow for TRE users to provision resources in terms of workspaces and workspace services. The Composition Service exposes resources \u2013 based on above concepts \u2013 as an HTTP API where users and applications can model the desired representation of the TRE, i.e., define which workspaces should contain which workspace services The Composition Service reconciles the desired state with the actual state by invoking Azure resource deployments.","title":"Composition Service"},{"location":"#services","text":"A service provide one or more capabilities to you as a user of the TRE or to the TRE itself. Depending on the type of the service it is scoped to the environment and shared across all workspaces (Shared Service) or scoped to a specific workspace (Workspace Service). The types of services required for a research project varies greatly why extensibility is a key aspect of the Azure TRE solution. New services can be developed by you and your organization to fit your needs. Some Workspace Services are accessible from outside the protected network, such as a Virtual Desktop. No data will be permitted to be transferred outside the protected network. Other services such as Azure Machine Learning might need access restricting to via a Virtual Desktop.","title":"Services"},{"location":"#shared-services","text":"Shared Services are services and resource shared by all workspaces. Firewall Package Mirror Git Mirror","title":"Shared Services"},{"location":"#workspace","text":"A workspace is a set of resources on a network with inbound traffic, restricted to authorised users, and outbound access restricted to defined network locations. The workspace is a security boundary and there should be zero transfer of data out from the workspace unless explicitly configured. Data transfer is not restricted within a workspace. The workspace itself contains only the bare essentials to provide this functionality, such as firewalls, storage etc. Workspaces can be enhanced with one or more building blocks called workspace services like Azure ML, Guacamole etc. to allow functionality such as development of machine learning models, data engineering, data analysis and software development. Multiple workspaces can be created within a single Trusted Research Environment to create the required separation for your projects. Each workspace has workspace users : one workspace owner, and one or more workspace researchers that can access the data and workspace services in the workspace. The workspace owner is also considered a workspace researcher.","title":"Workspace"},{"location":"#workspace-service","text":"A workspace service is a service, created as a building block, with pre-configured set of resources that can be applied to a workspace. Examples of Workspace Services are: Guacamole (Virtual Desktops) Azure Machine Learning Unlike shared services, a workspace service is only accessible to the workspace users. Some workspace services, such as Guacamole, allow users to add on user-specific resources (user resources) All workspace services can be deployed to all workspaces.","title":"Workspace Service"},{"location":"#user-resource","text":"A user resource is a resource that is only available to a particular researcher. For example a Guacamole VM. User resources can be deployed to workspaces with a compatible workspace service. E.g. Guacamole VMs can only be deployed to workspaces where the Guacamole workspace service is deployed.","title":"User Resource"},{"location":"#templates","text":"In order to deploy resources (workspaces, workspace services, user resources), the resources have to be defined in templates. A template contains everything needed to create an instance of the resource. Ex. a base workspace template, or a Guacamole workspace service template. The templates describe the porter bundles used, and the input parameters needed to deploy them. To use a template, and deploy a resource, the template needs to be registered in the TRE. This is done using the TRE API. Tip Once a template is registered it can be used multiple times to deploy multiple workspaces, workspace services etc. If you want to author your own workspace, workspace service, or user resource template, consult the template authoring guide","title":"Templates"},{"location":"deployment-quickstart/","text":"Deployment quickstart By following this quickstart you will deploy an Azure TRE instance for training and evaluation purposes using the minimum amount of steps and actions. Prerequisites To deploy an Azure TRE instance, there are a couple of prerequisites: Azure subscription Azure Active Directory tenant in which you can create application registrations. Git or GitHub Desktop The Azure TRE solution contains a development container with all the required tooling to develop and deploy the Azure TRE. To deploy an Azure TRE instance using the provided development container you will also need: Docker desktop Visual Studio Code Remote containers extension for Visual Studio Code . Clone the Azure TRE Git repository Clone the Azure TRE Git repository on GitHub to your local computer. > git clone https://github.com/microsoft/AzureTRE.git The Git repository will host some basic configuration for the TRE instances that are deployed from a given repository. Create a new branch for the instance that you are about to deploy. > cd AzureTRE AzureTRE> git checkout -b quickstartenv Now, let's open the cloned repository in Visual Studio Code and connect to the development container. AzureTRE> code . Tip Visual Studio Code should recognize the available development container and ask you to open the folder using it. For additional details on connecting to remote containers, please see the Open an existing folder in a container quickstart. When you start the development container for the first time, the container will be built. This usually takes a few minutes. Set environment configuration variables of shared management resources Open the /devops/.env.sample file and then save it without the .sample extension. You should now have a file called .env located in the devops folder. The /devops/.env file contains configuration variables for the shared management infrastructure which is used to support the deployment of one or more Azure TRE instances. You need to provide values for the following variables: VARIABLE DESCRIPTION LOCATION The Azure region to deploy to MGMT_RESOURCE_GROUP_NAME Resource group name MGMT_STORAGE_ACCOUNT_NAME Storage account name ACR_NAME Container registry name ARM_SUBSCRIPTION_ID Azure subscription id Comment out the following variables by starting the line with a hash # . # ARM_TENANT_ID=... # ARM_CLIENT_ID=... # ARM_CLIENT_SECRET=... The rest of the variables can have their default values. You should now have a .env file that looks similar to below. # Management infrastructure LOCATION=westeurope MGMT_RESOURCE_GROUP_NAME=aztremgmt MGMT_STORAGE_ACCOUNT_NAME=aztremgmt TERRAFORM_STATE_CONTAINER_NAME=tfstate IMAGE_TAG=dev ACR_NAME=aztreacr ARM_SUBSCRIPTION_ID=12...54e # Azure Resource Manager credentials used for CI/CD pipelines # ARM_TENANT_ID=__CHANGE_ME__ # ARM_CLIENT_ID=__CHANGE_ME__ # ARM_CLIENT_SECRET=__CHANGE_ME__ PORTER_OUTPUT_CONTAINER_NAME=porterout # Debug mode DEBUG=\"false\" Tip To retrieve your Azure subscription id, you can use the az command line interface available in the development container. In the terminal window in Visual Studio Code, type az login followed by az account show to see your default subscription. Please refer to az account -help for further details on how to change your active subscription if desired. Set environment configuration variables of the Azure TRE instance Next up, you will set the configuration variables for the specific Azure TRE instance. Open the /templates/core/.env.sample file and then save it without the .sample extension. You should now have a file called .env located in the /templates/core folder. The Azure TRE API is protected by Azure Active Directory. This requires an application registration for the API and another application registration for the Open API UI. Use the terminal window in Visual Studio Code to execute the following script from within the development container: /workspaces/tre> az login Note In case you have several subscriptions and would like to change your default subscription use az account set --subscription desired_subscription_id /workspaces/tre> ./scripts/aad-app-reg.sh -n aztreqs -r https://aztreqs.westeurope.cloudapp.azure.com/oidc-redirect -a Note aztreqs is a placeholder for the unique name you have to choose for your Azure TRE instance. Likewise westeurope is a placeholder for the location where the resources will be deployed, this should match the value you set on the location variable in the previous step. With the output from the aad-app-reg.sh script, you can now provide the required values for the following variables in the /templates/core/.env configuration file: VARIABLE DESCRIPTION TRE_ID The identifier for your Azure TRE instance. Will be used for naming Azure resources. Needs to be globally unique and less than 12 characters. AAD_TENANT_ID The Azure AD Tenant ID API_CLIENT_ID API application (client) ID API_CLIENT_SECRET API application client secret SWAGGER_UI_CLIENT_ID Swagger (OpenAPI) UI application (client) ID All other variables can have their default values for now. You should now have a .env file that looks similar to below. # Used for TRE deployment TRE_ID=aztreqs CORE_ADDRESS_SPACE=\"10.1.0.0/22\" TRE_ADDRESS_SPACE=\"10.0.0.0/12\" API_IMAGE_TAG=dev RESOURCE_PROCESSOR_VMSS_PORTER_IMAGE_TAG=dev GITEA_IMAGE_TAG=dev DEPLOY_GITEA=true RESOURCE_PROCESSOR_TYPE=\"vmss_porter\" # Auth configuration AAD_TENANT_ID=72e...45 API_CLIENT_ID=af6...dc API_CLIENT_SECRET=abc...12 SWAGGER_UI_CLIENT_ID=d87...12 Deploy the Azure TRE instance You are now ready to deploy the Azure TRE instance. Execute the all action of the makefile using make : /workspaces/tre> make all Deploying a new Azure TRE instance takes approximately 30 minutes. Once the deployment is completed you will be presented with a few output variables, similar to below. app_gateway_name = \"agw-aztreqs\" azure_tre_fqdn = \"aztreqs.westeurope.cloudapp.azure.com\" core_resource_group_name = \"rg-aztreqs\" keyvault_name = \"kv-aztreqs\" log_analytics_name = \"log-aztreqs\" static_web_storage = \"stwebaztreqs\" The Azure TRE instance is initially deployed with an invalid self-signed SSL certificate. This certificate needs to be replaced with one valid for your configured domain name. To use a certificate from Let's Encrypt , run the command: /workspaces/tre> make letsencrypt Validate the deployment Using curl Use curl to make a simple request to the status endpoint of the API: /workspaces/tre> curl https://<azure_tre_fqdn>/api/status The expected response is: { \"services\" :[{ \"service\" : \"Cosmos DB\" , \"status\" : \"OK\" , \"message\" : \"\" }]} Using the API docs Open your browser and navigate to the /api/docs route of the API: https://<azure_tre_fqdn>/api/docs and click Try it out on the operation of choice. Next steps Deploy a new workspace for Azure Machine Learning Enable users to access the Azure TRE instance Create a new workspace template","title":"Deployment Quickstart"},{"location":"deployment-quickstart/#deployment-quickstart","text":"By following this quickstart you will deploy an Azure TRE instance for training and evaluation purposes using the minimum amount of steps and actions.","title":"Deployment quickstart"},{"location":"deployment-quickstart/#prerequisites","text":"To deploy an Azure TRE instance, there are a couple of prerequisites: Azure subscription Azure Active Directory tenant in which you can create application registrations. Git or GitHub Desktop The Azure TRE solution contains a development container with all the required tooling to develop and deploy the Azure TRE. To deploy an Azure TRE instance using the provided development container you will also need: Docker desktop Visual Studio Code Remote containers extension for Visual Studio Code .","title":"Prerequisites"},{"location":"deployment-quickstart/#clone-the-azure-tre-git-repository","text":"Clone the Azure TRE Git repository on GitHub to your local computer. > git clone https://github.com/microsoft/AzureTRE.git The Git repository will host some basic configuration for the TRE instances that are deployed from a given repository. Create a new branch for the instance that you are about to deploy. > cd AzureTRE AzureTRE> git checkout -b quickstartenv Now, let's open the cloned repository in Visual Studio Code and connect to the development container. AzureTRE> code . Tip Visual Studio Code should recognize the available development container and ask you to open the folder using it. For additional details on connecting to remote containers, please see the Open an existing folder in a container quickstart. When you start the development container for the first time, the container will be built. This usually takes a few minutes.","title":"Clone the Azure TRE Git repository"},{"location":"deployment-quickstart/#set-environment-configuration-variables-of-shared-management-resources","text":"Open the /devops/.env.sample file and then save it without the .sample extension. You should now have a file called .env located in the devops folder. The /devops/.env file contains configuration variables for the shared management infrastructure which is used to support the deployment of one or more Azure TRE instances. You need to provide values for the following variables: VARIABLE DESCRIPTION LOCATION The Azure region to deploy to MGMT_RESOURCE_GROUP_NAME Resource group name MGMT_STORAGE_ACCOUNT_NAME Storage account name ACR_NAME Container registry name ARM_SUBSCRIPTION_ID Azure subscription id Comment out the following variables by starting the line with a hash # . # ARM_TENANT_ID=... # ARM_CLIENT_ID=... # ARM_CLIENT_SECRET=... The rest of the variables can have their default values. You should now have a .env file that looks similar to below. # Management infrastructure LOCATION=westeurope MGMT_RESOURCE_GROUP_NAME=aztremgmt MGMT_STORAGE_ACCOUNT_NAME=aztremgmt TERRAFORM_STATE_CONTAINER_NAME=tfstate IMAGE_TAG=dev ACR_NAME=aztreacr ARM_SUBSCRIPTION_ID=12...54e # Azure Resource Manager credentials used for CI/CD pipelines # ARM_TENANT_ID=__CHANGE_ME__ # ARM_CLIENT_ID=__CHANGE_ME__ # ARM_CLIENT_SECRET=__CHANGE_ME__ PORTER_OUTPUT_CONTAINER_NAME=porterout # Debug mode DEBUG=\"false\" Tip To retrieve your Azure subscription id, you can use the az command line interface available in the development container. In the terminal window in Visual Studio Code, type az login followed by az account show to see your default subscription. Please refer to az account -help for further details on how to change your active subscription if desired.","title":"Set environment configuration variables of shared management resources"},{"location":"deployment-quickstart/#set-environment-configuration-variables-of-the-azure-tre-instance","text":"Next up, you will set the configuration variables for the specific Azure TRE instance. Open the /templates/core/.env.sample file and then save it without the .sample extension. You should now have a file called .env located in the /templates/core folder. The Azure TRE API is protected by Azure Active Directory. This requires an application registration for the API and another application registration for the Open API UI. Use the terminal window in Visual Studio Code to execute the following script from within the development container: /workspaces/tre> az login Note In case you have several subscriptions and would like to change your default subscription use az account set --subscription desired_subscription_id /workspaces/tre> ./scripts/aad-app-reg.sh -n aztreqs -r https://aztreqs.westeurope.cloudapp.azure.com/oidc-redirect -a Note aztreqs is a placeholder for the unique name you have to choose for your Azure TRE instance. Likewise westeurope is a placeholder for the location where the resources will be deployed, this should match the value you set on the location variable in the previous step. With the output from the aad-app-reg.sh script, you can now provide the required values for the following variables in the /templates/core/.env configuration file: VARIABLE DESCRIPTION TRE_ID The identifier for your Azure TRE instance. Will be used for naming Azure resources. Needs to be globally unique and less than 12 characters. AAD_TENANT_ID The Azure AD Tenant ID API_CLIENT_ID API application (client) ID API_CLIENT_SECRET API application client secret SWAGGER_UI_CLIENT_ID Swagger (OpenAPI) UI application (client) ID All other variables can have their default values for now. You should now have a .env file that looks similar to below. # Used for TRE deployment TRE_ID=aztreqs CORE_ADDRESS_SPACE=\"10.1.0.0/22\" TRE_ADDRESS_SPACE=\"10.0.0.0/12\" API_IMAGE_TAG=dev RESOURCE_PROCESSOR_VMSS_PORTER_IMAGE_TAG=dev GITEA_IMAGE_TAG=dev DEPLOY_GITEA=true RESOURCE_PROCESSOR_TYPE=\"vmss_porter\" # Auth configuration AAD_TENANT_ID=72e...45 API_CLIENT_ID=af6...dc API_CLIENT_SECRET=abc...12 SWAGGER_UI_CLIENT_ID=d87...12","title":"Set environment configuration variables of the Azure TRE instance"},{"location":"deployment-quickstart/#deploy-the-azure-tre-instance","text":"You are now ready to deploy the Azure TRE instance. Execute the all action of the makefile using make : /workspaces/tre> make all Deploying a new Azure TRE instance takes approximately 30 minutes. Once the deployment is completed you will be presented with a few output variables, similar to below. app_gateway_name = \"agw-aztreqs\" azure_tre_fqdn = \"aztreqs.westeurope.cloudapp.azure.com\" core_resource_group_name = \"rg-aztreqs\" keyvault_name = \"kv-aztreqs\" log_analytics_name = \"log-aztreqs\" static_web_storage = \"stwebaztreqs\" The Azure TRE instance is initially deployed with an invalid self-signed SSL certificate. This certificate needs to be replaced with one valid for your configured domain name. To use a certificate from Let's Encrypt , run the command: /workspaces/tre> make letsencrypt","title":"Deploy the Azure TRE instance"},{"location":"deployment-quickstart/#validate-the-deployment","text":"","title":"Validate the deployment"},{"location":"deployment-quickstart/#using-curl","text":"Use curl to make a simple request to the status endpoint of the API: /workspaces/tre> curl https://<azure_tre_fqdn>/api/status The expected response is: { \"services\" :[{ \"service\" : \"Cosmos DB\" , \"status\" : \"OK\" , \"message\" : \"\" }]}","title":"Using curl"},{"location":"deployment-quickstart/#using-the-api-docs","text":"Open your browser and navigate to the /api/docs route of the API: https://<azure_tre_fqdn>/api/docs and click Try it out on the operation of choice.","title":"Using the API docs"},{"location":"deployment-quickstart/#next-steps","text":"Deploy a new workspace for Azure Machine Learning Enable users to access the Azure TRE instance Create a new workspace template","title":"Next steps"},{"location":"azure-tre-overview/architecture/","text":"Azure TRE Architecture The Azure Trusted Research Environment (TRE) consists of multiple components, all encapsulated in networks with restricted ingress- & egress traffic. There is one network for the management components and one network per Workspace. All traffic has to be explicitly allowed by the Application Gateway or the Firewall. The Azure TRE management plane consists of two groups of components: API & Composition Service Shared Services Todo Shared Services is still work in progress. Please see #23 , #22 , & #21 The TRE API is a service that users can interact with to request changes to workspaces e.g., to create, update, delete workspaces and workspace services inside each workspace. The Composition Service is doing the actual work of mutating the state of each Workspace including the Workspace Services. Ingress/egress components governs all inbound and outbound traffic from the public Internet to and from Azure TRE including the Workspaces. The Firewall Service is managing the egress rules of the Firewall. Shared Services are services available to all Workspaces. Source Mirror can mirror source repositories such as GitHub, but only allowing read-access, hence data from a Workspace cannot be pushed to a source repository. Package Mirror is also a read-only front for developer/researcher application package services like NPM, PyPI, and NuGet and operating system application package services like apt-get and Windows Package Manager (winget). Composition Service The Composition Service is responsible for managing and mutating Workspaces and Workspace Services. A Workspace is an instance of a Workspace Template. A Workspace Template is implemented as a Porter bundle - read more about Authoring workspaces templates . A Porter bundle is a fully encapsulated versioned bundle with everything needed (binaries, scripts, IoC templates etc.) to provision an instance of Workspace Template. The TRE Administrator can register a Porter bundle to use the Composition Service to provision instances of the Workspace Templates. This requires: The Porter bundle to be pushed to the Azure Container Registry (ACR). Registering the Workspace through the API. Details on how to register a Workspace Template . The Composition Service consists of multiple components. Component Name Responsibility / Description TRE API An API responsible for performing all operations on Workspaces and managing Workspace Templates. Configuration Store Keeping the state of Workspaces and Workspace Templates. The store uses Cosmos DB (SQL) . Service Bus Azure Service Bus responsible for reliable delivery of messages between components. Resource Processor Responsible for starting the process of mutating a Workspace via a Workspace Template. Provisioning a Workspace The flow to provision a Workspace is as follows (the flow is the same for all kinds of mutations to a Workspace): An HTTP request to the TRE API to create a new Workspace. The request contains information like the name of the Workspace, the Workspace Template to use, and the parameters required for the Workspace Template (Workspace Templates can expose the parameters via a JSON Schema ). The desired state of the Workspace is updated in the Configuration Store. A command message with the Workspace Template reference and parameters are sent to the workspacequeue . { \"action\" : \"install\" , \"id\" : \"base\" , \"name\" : \"BaseWorkspaceTemplate\" , \"version\" : \"1.0\" , \"parameters\" : { \"param1\" : \"value1\" } } The Resource Processor picks up the new message from the service bus queue. The Resource Processor processes the command by executing the Porter bundle (the implementation of a Workspace Template). # simplified for readability porter <action> --reference <ACR name>.azurecr.io/bundles/<name>:<version> --params key = value --cred <credentials set name or file> # Example porter install --reference msfttreacr.azurecr.io/bundles/BaseWorkspaceTemplate:1.0 --params param1 = value1 --cred azure.json Deployments are carried out against the Azure Subscription using a User Assigned Managed Identity. The azure.json tells Porter where the credential information can be found and for the Resource Processor they are set as environment variables. Porter bundle actions are required to be idempotent, so if a deployment fails, the Resource Processor can retry. The Porter Docker bundle is pulled from the Azure Container Registry (ACR) and executed. The Porter bundle executes against Azure Resource Manager to provision Azure resources. Any kind of infrastructure of code frameworks like ARM, Terraform, or Pulumi can be used or scripted via PowerShell or Azure CLI. State and output management is handled via Azure Storage Containers. State for keeping persistent state between executions of a bundled with the same Workspace. For the time being, the Porter bundle updates Firewall rules directly setting egress rules. An enhancement to implement a Shared Firewall services is planned ( #23 ). The Resource Processor sends events to the deploymentstatus queue on state changes and informs if the deployment succeeded or failed. The status of a Porter bundle execution is received. The status of a Porter bundle execution is updated in the Configuration Store. Info The Resource Processor is a Docker container running on a Linux VM scale set. Todo Currently, the bundle keeps state between executions in a Storage Container (TF state) passed in a parameters to the bundle. An enhancement issues #536 exists to configure Porter state management.","title":"Architecture"},{"location":"azure-tre-overview/architecture/#azure-tre-architecture","text":"The Azure Trusted Research Environment (TRE) consists of multiple components, all encapsulated in networks with restricted ingress- & egress traffic. There is one network for the management components and one network per Workspace. All traffic has to be explicitly allowed by the Application Gateway or the Firewall. The Azure TRE management plane consists of two groups of components: API & Composition Service Shared Services Todo Shared Services is still work in progress. Please see #23 , #22 , & #21 The TRE API is a service that users can interact with to request changes to workspaces e.g., to create, update, delete workspaces and workspace services inside each workspace. The Composition Service is doing the actual work of mutating the state of each Workspace including the Workspace Services. Ingress/egress components governs all inbound and outbound traffic from the public Internet to and from Azure TRE including the Workspaces. The Firewall Service is managing the egress rules of the Firewall. Shared Services are services available to all Workspaces. Source Mirror can mirror source repositories such as GitHub, but only allowing read-access, hence data from a Workspace cannot be pushed to a source repository. Package Mirror is also a read-only front for developer/researcher application package services like NPM, PyPI, and NuGet and operating system application package services like apt-get and Windows Package Manager (winget).","title":"Azure TRE Architecture"},{"location":"azure-tre-overview/architecture/#composition-service","text":"The Composition Service is responsible for managing and mutating Workspaces and Workspace Services. A Workspace is an instance of a Workspace Template. A Workspace Template is implemented as a Porter bundle - read more about Authoring workspaces templates . A Porter bundle is a fully encapsulated versioned bundle with everything needed (binaries, scripts, IoC templates etc.) to provision an instance of Workspace Template. The TRE Administrator can register a Porter bundle to use the Composition Service to provision instances of the Workspace Templates. This requires: The Porter bundle to be pushed to the Azure Container Registry (ACR). Registering the Workspace through the API. Details on how to register a Workspace Template . The Composition Service consists of multiple components. Component Name Responsibility / Description TRE API An API responsible for performing all operations on Workspaces and managing Workspace Templates. Configuration Store Keeping the state of Workspaces and Workspace Templates. The store uses Cosmos DB (SQL) . Service Bus Azure Service Bus responsible for reliable delivery of messages between components. Resource Processor Responsible for starting the process of mutating a Workspace via a Workspace Template.","title":"Composition Service"},{"location":"azure-tre-overview/architecture/#provisioning-a-workspace","text":"The flow to provision a Workspace is as follows (the flow is the same for all kinds of mutations to a Workspace): An HTTP request to the TRE API to create a new Workspace. The request contains information like the name of the Workspace, the Workspace Template to use, and the parameters required for the Workspace Template (Workspace Templates can expose the parameters via a JSON Schema ). The desired state of the Workspace is updated in the Configuration Store. A command message with the Workspace Template reference and parameters are sent to the workspacequeue . { \"action\" : \"install\" , \"id\" : \"base\" , \"name\" : \"BaseWorkspaceTemplate\" , \"version\" : \"1.0\" , \"parameters\" : { \"param1\" : \"value1\" } } The Resource Processor picks up the new message from the service bus queue. The Resource Processor processes the command by executing the Porter bundle (the implementation of a Workspace Template). # simplified for readability porter <action> --reference <ACR name>.azurecr.io/bundles/<name>:<version> --params key = value --cred <credentials set name or file> # Example porter install --reference msfttreacr.azurecr.io/bundles/BaseWorkspaceTemplate:1.0 --params param1 = value1 --cred azure.json Deployments are carried out against the Azure Subscription using a User Assigned Managed Identity. The azure.json tells Porter where the credential information can be found and for the Resource Processor they are set as environment variables. Porter bundle actions are required to be idempotent, so if a deployment fails, the Resource Processor can retry. The Porter Docker bundle is pulled from the Azure Container Registry (ACR) and executed. The Porter bundle executes against Azure Resource Manager to provision Azure resources. Any kind of infrastructure of code frameworks like ARM, Terraform, or Pulumi can be used or scripted via PowerShell or Azure CLI. State and output management is handled via Azure Storage Containers. State for keeping persistent state between executions of a bundled with the same Workspace. For the time being, the Porter bundle updates Firewall rules directly setting egress rules. An enhancement to implement a Shared Firewall services is planned ( #23 ). The Resource Processor sends events to the deploymentstatus queue on state changes and informs if the deployment succeeded or failed. The status of a Porter bundle execution is received. The status of a Porter bundle execution is updated in the Configuration Store. Info The Resource Processor is a Docker container running on a Linux VM scale set. Todo Currently, the bundle keeps state between executions in a Storage Container (TF state) passed in a parameters to the bundle. An enhancement issues #536 exists to configure Porter state management.","title":"Provisioning a Workspace"},{"location":"azure-tre-overview/networking/","text":"Network Architecture The Trusted Research Environment (TRE) network topology is based on hub-spoke . The TRE Management VNET ( Azure Virtual Network ) is the central hub and each workspace is a spoke. Note TRE Management is referred to as core in scripts and code. Azure TRE VNETs are segregated allowing limited traffic between the TRE Management VNET and Workspace VNETs. The security rules are managed by nsg-ws network security group. See workspace network security groups (NSG) further down. The Core VNET is further divided into subnets. Subnet Description AzureBastionSubnet A dedicated subnet for Azure Bastion hosts. AppGwSubnet Subnet for Azure Application Gateway controlling ingress traffic. AzureFirewallSubnet Subnet for Azure Firewall controlling egress traffic. ResourceProcessorSubnet Subnet for VMSS used by the Composition Service to host Docker containers to execute Porter bundles that deploys Workspaces. WebAppSubnet Subnet for TRE API. SharedSubnet Shared Services subnet for all things shared by TRE Core and Workspaces. Such as Source Mirror Shared Service and Package Mirror Shared Service. All subnets (Core and Workspace subnets) has a default route which directs egress traffic to the Azure Firewall, to ensure only explicitly allowed destinations on the Internet to be accessed. There are a couple of exceptions AzureFirewallSubnet as it hosts the Azure Firewall which routes traffic to the Internet. AzureBastionSubnet as it hosts Azure Bastion which is the management jump box within the VNET with Internet access. AppGwSubnet as it hosts the Azure Application Gateway which has to be able to a ping the health endpoints e.g. TRE API. Ingress and egress Ingress traffic from the Internet is only allowed through the Application Gateway, which forwards HTTPS (port 443) call to the TRE API in the WebAppSubnet . Egress traffic is routed through the Azure Firewall with a few exceptions and by default all ingress and egress traffic is denied except explicitly allowed. The explicitly allowed egress traffic is described here: Resource Processor TRE API Gitea Shared Service Nexus Shared Service Network security groups TRE Management/core Network security groups (NSG), and their security rules for TRE core resources are defined in /templates/core/terraform/network/network_security_groups.tf . Network security group Associated subnet(s) nsg-bastion-subnet AzureBastionSubnet nsg-app-gw AppGwSubnet nsg-default-rules ResourceProcessorSubnet , SharedSubnet , WebAppSubnet Workspaces Azure TRE VNETs are segregated allowing limited traffic between the TRE Management VNET and Workspace VNETs. The rules to manage and limit the traffic between the TRE Management VNET and Workspace VNETs are defined by the nsg-ws network security group: Inbound traffic from TRE Management VNET to workspace allowed for Azure Bastion (22, 3389) - All other inbound traffic from Core to workspace denied. Outbound traffic to SharedSubnet from Workspace allowed. Outbound traffic to Internet allowed on HTTPS port 443 (next hop Azure Firewall). All other outbound traffic denied. Each of these rules can be managed per workspace. Caution In Azure, traffic between subnets are allowed except explicitly denied.","title":"Network Architecture"},{"location":"azure-tre-overview/networking/#network-architecture","text":"The Trusted Research Environment (TRE) network topology is based on hub-spoke . The TRE Management VNET ( Azure Virtual Network ) is the central hub and each workspace is a spoke. Note TRE Management is referred to as core in scripts and code. Azure TRE VNETs are segregated allowing limited traffic between the TRE Management VNET and Workspace VNETs. The security rules are managed by nsg-ws network security group. See workspace network security groups (NSG) further down. The Core VNET is further divided into subnets. Subnet Description AzureBastionSubnet A dedicated subnet for Azure Bastion hosts. AppGwSubnet Subnet for Azure Application Gateway controlling ingress traffic. AzureFirewallSubnet Subnet for Azure Firewall controlling egress traffic. ResourceProcessorSubnet Subnet for VMSS used by the Composition Service to host Docker containers to execute Porter bundles that deploys Workspaces. WebAppSubnet Subnet for TRE API. SharedSubnet Shared Services subnet for all things shared by TRE Core and Workspaces. Such as Source Mirror Shared Service and Package Mirror Shared Service. All subnets (Core and Workspace subnets) has a default route which directs egress traffic to the Azure Firewall, to ensure only explicitly allowed destinations on the Internet to be accessed. There are a couple of exceptions AzureFirewallSubnet as it hosts the Azure Firewall which routes traffic to the Internet. AzureBastionSubnet as it hosts Azure Bastion which is the management jump box within the VNET with Internet access. AppGwSubnet as it hosts the Azure Application Gateway which has to be able to a ping the health endpoints e.g. TRE API.","title":"Network Architecture"},{"location":"azure-tre-overview/networking/#ingress-and-egress","text":"Ingress traffic from the Internet is only allowed through the Application Gateway, which forwards HTTPS (port 443) call to the TRE API in the WebAppSubnet . Egress traffic is routed through the Azure Firewall with a few exceptions and by default all ingress and egress traffic is denied except explicitly allowed. The explicitly allowed egress traffic is described here: Resource Processor TRE API Gitea Shared Service Nexus Shared Service","title":"Ingress and egress"},{"location":"azure-tre-overview/networking/#network-security-groups","text":"","title":"Network security groups"},{"location":"azure-tre-overview/networking/#tre-managementcore","text":"Network security groups (NSG), and their security rules for TRE core resources are defined in /templates/core/terraform/network/network_security_groups.tf . Network security group Associated subnet(s) nsg-bastion-subnet AzureBastionSubnet nsg-app-gw AppGwSubnet nsg-default-rules ResourceProcessorSubnet , SharedSubnet , WebAppSubnet","title":"TRE Management/core"},{"location":"azure-tre-overview/networking/#workspaces","text":"Azure TRE VNETs are segregated allowing limited traffic between the TRE Management VNET and Workspace VNETs. The rules to manage and limit the traffic between the TRE Management VNET and Workspace VNETs are defined by the nsg-ws network security group: Inbound traffic from TRE Management VNET to workspace allowed for Azure Bastion (22, 3389) - All other inbound traffic from Core to workspace denied. Outbound traffic to SharedSubnet from Workspace allowed. Outbound traffic to Internet allowed on HTTPS port 443 (next hop Azure Firewall). All other outbound traffic denied. Each of these rules can be managed per workspace. Caution In Azure, traffic between subnets are allowed except explicitly denied.","title":"Workspaces"},{"location":"azure-tre-overview/user-roles/","text":"User roles The Azure TRE solution has 8 different user roles defined. The roles are modeled around a set of tasks for each role. The roles are not mutually exclusive, and one person can have multiple roles assigned to be able to carry out a broader set of tasks. Before you deploy a Trusted Research Environment based on the Azure TRE solution, you should consider your scenario and have an understanding of which of these roles that needs to be staffed. Role overview While we have defined 8 different user roles for the Azure TRE solution, not all of them are required in all scenarios. Three of the roles support role-based access control (RBAC) within the TRE. Role Key task TRE RBAC Azure administrator Deploy the TRE TRE administrator Administer the TRE \u2714 TRE workspace owner Own a workspace \u2714 Researcher Perform research on the data \u2714 TRE service integrator Integrate additional workspace services Azure TRE developer Extend the TRE OSS solution Data engineer Move data to and potentially from the TRE Information security officer Validate and sign-off TRE deployment Azure administrator Provisions the Azure TRE solution in an Azure subscription and performs tasks that require knowledge of Azure operations and has access to the Azure subscription. Example tasks: Provision Azure TRE solution instances. Second line support for TRE administrators, TRE workspace owners and Researchers when Azure TRE troubleshooting is required. Work with the data engineer to connect the Azure TRE with the data platform. Troubleshoot provisioning issues and failed deployments. Manage TRE administrator users. Manage data backups and restores. Update the Azure TRE instances. Configure log and metrics alerts. Expected skills: Azure administration and operations. Infrastructure as Code (Terraform, ARM, Git) PowerShell, Bash TRE administrator Day-to-day running and operations of the Azure TRE instance without touching Azure resources. Example tasks: Manage workspace owner users. Provision workspaces. Manage shared services e.g., available packages in package mirror shared service. Monitor workspace usage and billing. Set and manage quotas. Create and manage workspaces Expected skills: Limited or no Azure knowledge expected. TRE workspace owner Owns a specific workspace and has additional privileges than the researcher within the workspace. Is most likely also a Researcher . Example tasks: Manage Researcher users. Export data from workspace. Import data and make it available within the workspace. Enable services within the workspace. Monitor billing and usage of the workspace. Create and manage workspace services Expected skills: Limited or no Azure knowledge expected. Researcher Has access to one specific workspace and can use all the services provisioned within that workspace. Example tasks: Import software packages needed to conduct research (PyPi, Conda, Apt). Perform research using the services in the workspace. Create and manage user resources Expected skills: Python, R Git Linux TRE service integrator Integrates workspace service types with an Azure TRE instance. This involves extending the Azure Infrastructure as Code templates to make a workspace service available within an Azure TRE instance. Example tasks: Integrate a workspace service type with your Azure TRE instance. Implement Infrastructure as Code templates for new workspace service types. Expected skills: Infrastructure as Code (Terraform, ARM, Git) Python, C#, PowerShell, Bash Azure administration Azure TRE developer Software developer who contributes to the development of the Azure TRE solution. Example tasks: Modify the deployment service, API and other components of the Azure TRE solution. Contribute to the Azure TRE OSS solution. Expected skills: Infrastructure as Code (Terraform, ARM, Git) Python, C#, PowerShell, Bash Azure administration Data engineer Supporting role that is expected to build data movement pipelines between the data platform (not part of the TRE), and the TRE instance. Example tasks: Transfer data from the data platform to the TRE and potentially back. Create data movement and transformation pipelines. Expected skills: Python, Bash, Linux Azure Data Factory, Other ETL tools. Information Security Officer Needs to understand the security posture of the TRE to ensure that the organization is compliant with the information governance framework and additional relevant regulations. Example tasks: Use the Azure TRE documentation to understand the security posture of the TRE. Work with Azure administrator and TRE administrator to enforce the required security and privacy controls on the TRE. Commission penetration testing. Work with organization Information Governance committee to validate and sign-off Azure TRE deployment","title":"User Roles"},{"location":"azure-tre-overview/user-roles/#user-roles","text":"The Azure TRE solution has 8 different user roles defined. The roles are modeled around a set of tasks for each role. The roles are not mutually exclusive, and one person can have multiple roles assigned to be able to carry out a broader set of tasks. Before you deploy a Trusted Research Environment based on the Azure TRE solution, you should consider your scenario and have an understanding of which of these roles that needs to be staffed.","title":"User roles"},{"location":"azure-tre-overview/user-roles/#role-overview","text":"While we have defined 8 different user roles for the Azure TRE solution, not all of them are required in all scenarios. Three of the roles support role-based access control (RBAC) within the TRE. Role Key task TRE RBAC Azure administrator Deploy the TRE TRE administrator Administer the TRE \u2714 TRE workspace owner Own a workspace \u2714 Researcher Perform research on the data \u2714 TRE service integrator Integrate additional workspace services Azure TRE developer Extend the TRE OSS solution Data engineer Move data to and potentially from the TRE Information security officer Validate and sign-off TRE deployment","title":"Role overview"},{"location":"azure-tre-overview/user-roles/#azure-administrator","text":"Provisions the Azure TRE solution in an Azure subscription and performs tasks that require knowledge of Azure operations and has access to the Azure subscription. Example tasks: Provision Azure TRE solution instances. Second line support for TRE administrators, TRE workspace owners and Researchers when Azure TRE troubleshooting is required. Work with the data engineer to connect the Azure TRE with the data platform. Troubleshoot provisioning issues and failed deployments. Manage TRE administrator users. Manage data backups and restores. Update the Azure TRE instances. Configure log and metrics alerts. Expected skills: Azure administration and operations. Infrastructure as Code (Terraform, ARM, Git) PowerShell, Bash","title":"Azure administrator"},{"location":"azure-tre-overview/user-roles/#tre-administrator","text":"Day-to-day running and operations of the Azure TRE instance without touching Azure resources. Example tasks: Manage workspace owner users. Provision workspaces. Manage shared services e.g., available packages in package mirror shared service. Monitor workspace usage and billing. Set and manage quotas. Create and manage workspaces Expected skills: Limited or no Azure knowledge expected.","title":"TRE administrator"},{"location":"azure-tre-overview/user-roles/#tre-workspace-owner","text":"Owns a specific workspace and has additional privileges than the researcher within the workspace. Is most likely also a Researcher . Example tasks: Manage Researcher users. Export data from workspace. Import data and make it available within the workspace. Enable services within the workspace. Monitor billing and usage of the workspace. Create and manage workspace services Expected skills: Limited or no Azure knowledge expected.","title":"TRE workspace owner"},{"location":"azure-tre-overview/user-roles/#researcher","text":"Has access to one specific workspace and can use all the services provisioned within that workspace. Example tasks: Import software packages needed to conduct research (PyPi, Conda, Apt). Perform research using the services in the workspace. Create and manage user resources Expected skills: Python, R Git Linux","title":"Researcher"},{"location":"azure-tre-overview/user-roles/#tre-service-integrator","text":"Integrates workspace service types with an Azure TRE instance. This involves extending the Azure Infrastructure as Code templates to make a workspace service available within an Azure TRE instance. Example tasks: Integrate a workspace service type with your Azure TRE instance. Implement Infrastructure as Code templates for new workspace service types. Expected skills: Infrastructure as Code (Terraform, ARM, Git) Python, C#, PowerShell, Bash Azure administration","title":"TRE service integrator"},{"location":"azure-tre-overview/user-roles/#azure-tre-developer","text":"Software developer who contributes to the development of the Azure TRE solution. Example tasks: Modify the deployment service, API and other components of the Azure TRE solution. Contribute to the Azure TRE OSS solution. Expected skills: Infrastructure as Code (Terraform, ARM, Git) Python, C#, PowerShell, Bash Azure administration","title":"Azure TRE developer"},{"location":"azure-tre-overview/user-roles/#data-engineer","text":"Supporting role that is expected to build data movement pipelines between the data platform (not part of the TRE), and the TRE instance. Example tasks: Transfer data from the data platform to the TRE and potentially back. Create data movement and transformation pipelines. Expected skills: Python, Bash, Linux Azure Data Factory, Other ETL tools.","title":"Data engineer"},{"location":"azure-tre-overview/user-roles/#information-security-officer","text":"Needs to understand the security posture of the TRE to ensure that the organization is compliant with the information governance framework and additional relevant regulations. Example tasks: Use the Azure TRE documentation to understand the security posture of the TRE. Work with Azure administrator and TRE administrator to enforce the required security and privacy controls on the TRE. Commission penetration testing. Work with organization Information Governance committee to validate and sign-off Azure TRE deployment","title":"Information Security Officer"},{"location":"azure-tre-overview/shared-services/gitea/","text":"Gitea Shared Service As outbound access to public git repositories such as GitHub is often blocked a Git mirror may be required. Gitea can be deployed as a shared service to offer this functionality. Documentation on Gitea can be found here: https://docs.gitea.io/ . Deploy To deploy set DEPLOY_GITEA=true in templates/core/.env Getting Started In order to connect to the gitea admin console use the user \"gitea_admin\". The user's password can be found in keyvault as gitea password. Network requirements To be able to run the Gitea Shared Service it needs to be able to access the following resource outside the Azure TRE VNET via explicit allowed Service Tags or URLs. Service Tag / Destination Justification AzureActiveDirectory Authorize the signed in user against Azure Active Directory. AzureContainerRegistry Pull the Gitea container image, as it is located in Azure Container Registry. AzureMonitor Forwards tracing an logs to central location for troubleshooting. (www.)github.com Allows Gitea to mirror any repo on GitHub","title":"Gitea (Source Mirror)"},{"location":"azure-tre-overview/shared-services/gitea/#gitea-shared-service","text":"As outbound access to public git repositories such as GitHub is often blocked a Git mirror may be required. Gitea can be deployed as a shared service to offer this functionality. Documentation on Gitea can be found here: https://docs.gitea.io/ .","title":"Gitea Shared Service"},{"location":"azure-tre-overview/shared-services/gitea/#deploy","text":"To deploy set DEPLOY_GITEA=true in templates/core/.env","title":"Deploy"},{"location":"azure-tre-overview/shared-services/gitea/#getting-started","text":"In order to connect to the gitea admin console use the user \"gitea_admin\". The user's password can be found in keyvault as gitea password.","title":"Getting Started"},{"location":"azure-tre-overview/shared-services/gitea/#network-requirements","text":"To be able to run the Gitea Shared Service it needs to be able to access the following resource outside the Azure TRE VNET via explicit allowed Service Tags or URLs. Service Tag / Destination Justification AzureActiveDirectory Authorize the signed in user against Azure Active Directory. AzureContainerRegistry Pull the Gitea container image, as it is located in Azure Container Registry. AzureMonitor Forwards tracing an logs to central location for troubleshooting. (www.)github.com Allows Gitea to mirror any repo on GitHub","title":"Network requirements"},{"location":"azure-tre-overview/shared-services/nexus/","text":"Nexus Shared Service This service allows users in workspaces to access external software packages securely by relying on Sonatype Nexus (RepoManager). Documentation on Nexus can be found here: https://help.sonatype.com/repomanager3/ . Deploy To deploy set DEPLOY_NEXUS=true in templates/core/.env . Configure Wait for the application to come online - it can take a few minutes... and then navigate to its homepage. You can find the url by looking at the management resource group and finding a web application whose name start with nexus. Retrieve the initial admin password from the \"admin.password\" file. You will find it by going to TRE management resource group -> storage account named \"stg\\ \" -> \"File Shares\" -> \"nexus-data\". Use the password to login to Nexus and go through the initial setup wizard. You can allow anonymous access because the purpose of this service is to use publicly available software packages. On the admin screen, add proxy repositories as needed. Note that other types of repositories might be a way to move data in/out workspaces, and you should not allow that. Finally, share the repositories addresses with your users. Network requirements To be able to run the Nexus Shared Service it needs to be able to access the following resource outside the Azure TRE VNET via explicit allowed Service Tags or URLs. Service Tag / Destination Justification AzureActiveDirectory Authorize the signed in user against Azure Active Directory. AzureContainerRegistry Pull the Nexus container image, as it is located in Azure Container Registry. AzureMonitor Forwards tracing an logs to central location for troubleshooting. pypi.org Enables Nexus to \"proxy\" python packages to use inside of workspaces","title":"Nexus (Package Mirror)"},{"location":"azure-tre-overview/shared-services/nexus/#nexus-shared-service","text":"This service allows users in workspaces to access external software packages securely by relying on Sonatype Nexus (RepoManager). Documentation on Nexus can be found here: https://help.sonatype.com/repomanager3/ .","title":"Nexus Shared Service"},{"location":"azure-tre-overview/shared-services/nexus/#deploy","text":"To deploy set DEPLOY_NEXUS=true in templates/core/.env .","title":"Deploy"},{"location":"azure-tre-overview/shared-services/nexus/#configure","text":"Wait for the application to come online - it can take a few minutes... and then navigate to its homepage. You can find the url by looking at the management resource group and finding a web application whose name start with nexus. Retrieve the initial admin password from the \"admin.password\" file. You will find it by going to TRE management resource group -> storage account named \"stg\\ \" -> \"File Shares\" -> \"nexus-data\". Use the password to login to Nexus and go through the initial setup wizard. You can allow anonymous access because the purpose of this service is to use publicly available software packages. On the admin screen, add proxy repositories as needed. Note that other types of repositories might be a way to move data in/out workspaces, and you should not allow that. Finally, share the repositories addresses with your users.","title":"Configure"},{"location":"azure-tre-overview/shared-services/nexus/#network-requirements","text":"To be able to run the Nexus Shared Service it needs to be able to access the following resource outside the Azure TRE VNET via explicit allowed Service Tags or URLs. Service Tag / Destination Justification AzureActiveDirectory Authorize the signed in user against Azure Active Directory. AzureContainerRegistry Pull the Nexus container image, as it is located in Azure Container Registry. AzureMonitor Forwards tracing an logs to central location for troubleshooting. pypi.org Enables Nexus to \"proxy\" python packages to use inside of workspaces","title":"Network requirements"},{"location":"tre-admins/troubleshooting-guide/","text":"Operations Debugging and Troubleshooting guide This guide explains how to go about finding the root cause of why a workspace resource might not have been deployed. The steps listed below should be followed in order as that is how the message also flows in the system. Enabling DEBUG mode on the API The API is by default configured to not show detailed error messages and stack trace when an error occurs. This is done to prevent leaking internal state to the outside world and to minimize information which an attacker could use against the deployed instance. However, you can enable DEBUG=true in the configuration settings of the API using Azure portal. Go to App Service for the API and select Settings > Configuration. Click New Application Setting. in the new dialog box set Name = DEBUG and Value = true With DEBUG mode enabled when an error occurs at the API level it will display a detailed error message which should help in understanding why the payload was not accepted. API logs using deployment center You should also check that the version you are debugging/troubleshooting is the actual one deployed on the App Service. This can be checked using Deployment Center. You can also follow the logs as generated by the container in the logs tabs. Checking the Service Bus If the message payload is accepted by the API and a workspace_id is generated you should be able to track the progress of the deployment using GET /api/workspaces/{workspace_id} To start with the status is always reported as \"deployment\" : { \"status\" : \"not_deployed\" , \"message\" : \"This resource has not yet been deployed\" } which should eventually change as the message flows through the system. If the message remains at this stage you should first verify that the message arrived in the service bus. This is most easily done on the Azure portal. Select the Service Bus from deployed resources and click Entities > Queues > workspacequeue. Select the Service Bus Explorer and the Peek tab to check for hanging messages. Checking the logs in App Insights Every component of TRE should send their trace logs to App Insights logging backend, and you should be able to see the process flow by using a suitable query. Traces however take time to appear so be patient. Go to the deployed app insights instance and select Monitoring > Logs where you can run the following example query to get the logs for a specific deployment. let tracking_id=\"<workspace_id>\"; traces | where message contains tracking_id or operation_Id == tracking_id | sort by timestamp desc For a successful deployment you should see the last message (at the top since the order is timestamp descending) something like Received deployment status update message with correlation ID 28fcf096-6962-498b-9d2e-16d38dc6b7f6: {'id': '28fcf096-6962-498b-9d2e-16d38dc6b7f6', 'status': 'deployed', 'message': 'cse-msr-dev-b7f6: Workspace was deployed successfully...'} It should also be evident from the message flow where the current processing is stuck or failed. Failed deployment status should also be available in the GET /api/workspaces/{workspace_id} and this is just another way to confirm it. Checking the Virtual Machine Scale Set(VMSS) instance running resource processor If you see messages hanging in the service bus queue then the resource processor is not up and running. Verify that the VMSS instance is up and healthy. The processor runs in a vnet, and you cannot connect to it directly. If the instance is up then you need to connect to the instance using Bastion. Bastion is already deployed, and you can use the username adminuser and the password is stored in the keyvault under the secret resource-processor-vmss-password Info You cannot see secrets unless you are added to a suitable Access Policy for the keyvault. After logging in you should check the status of cloud-init which is used to bootstrap the machine with docker and start the processor. Log files for cloud init are /var/log/cloud-init.log /var/log/cloud-init-output.log If the docker container is pulled as shown in logs then the resource processor should start. You can check the status of the container using docker ps However, if you see nothing (and the container was pulled) then the processor has either not started yet or it has crashed. You can check all docker processes using docker ps -a which should show if the container terminated before. You can get the logs from container using docker logs <container_id> command. If you want to start a processor container manually you can use the following command and execute from root(/) of filesystem. docker run -v /var/run/docker.sock:/var/run/docker.sock --env-file .env --name resource_processor_vmss_porter_debug runner_image:tag runner_image:tag can be obtained using docker ps Info All logs which you see from the resource processor should also be transferred to the App Insights instance as noted above, so it is not essential to follow the progress by logging into the instance. Logging into the instance and starting a container manually is helpful in live debugging. Updating the running container If you start a container manually you will probably want to install software, for example, an editor. However, firewall blocks all ingress traffic, so you cannot run sudo apt update . You need to add an override rule in the firewall to allow the traffic. Remember to remove this rule when debugging is done.","title":"Troubleshooting Guide"},{"location":"tre-admins/troubleshooting-guide/#operations-debugging-and-troubleshooting-guide","text":"This guide explains how to go about finding the root cause of why a workspace resource might not have been deployed. The steps listed below should be followed in order as that is how the message also flows in the system.","title":"Operations Debugging and Troubleshooting guide"},{"location":"tre-admins/troubleshooting-guide/#enabling-debug-mode-on-the-api","text":"The API is by default configured to not show detailed error messages and stack trace when an error occurs. This is done to prevent leaking internal state to the outside world and to minimize information which an attacker could use against the deployed instance. However, you can enable DEBUG=true in the configuration settings of the API using Azure portal. Go to App Service for the API and select Settings > Configuration. Click New Application Setting. in the new dialog box set Name = DEBUG and Value = true With DEBUG mode enabled when an error occurs at the API level it will display a detailed error message which should help in understanding why the payload was not accepted.","title":"Enabling DEBUG mode on the API"},{"location":"tre-admins/troubleshooting-guide/#api-logs-using-deployment-center","text":"You should also check that the version you are debugging/troubleshooting is the actual one deployed on the App Service. This can be checked using Deployment Center. You can also follow the logs as generated by the container in the logs tabs.","title":"API logs using deployment center"},{"location":"tre-admins/troubleshooting-guide/#checking-the-service-bus","text":"If the message payload is accepted by the API and a workspace_id is generated you should be able to track the progress of the deployment using GET /api/workspaces/{workspace_id} To start with the status is always reported as \"deployment\" : { \"status\" : \"not_deployed\" , \"message\" : \"This resource has not yet been deployed\" } which should eventually change as the message flows through the system. If the message remains at this stage you should first verify that the message arrived in the service bus. This is most easily done on the Azure portal. Select the Service Bus from deployed resources and click Entities > Queues > workspacequeue. Select the Service Bus Explorer and the Peek tab to check for hanging messages.","title":"Checking the Service Bus"},{"location":"tre-admins/troubleshooting-guide/#checking-the-logs-in-app-insights","text":"Every component of TRE should send their trace logs to App Insights logging backend, and you should be able to see the process flow by using a suitable query. Traces however take time to appear so be patient. Go to the deployed app insights instance and select Monitoring > Logs where you can run the following example query to get the logs for a specific deployment. let tracking_id=\"<workspace_id>\"; traces | where message contains tracking_id or operation_Id == tracking_id | sort by timestamp desc For a successful deployment you should see the last message (at the top since the order is timestamp descending) something like Received deployment status update message with correlation ID 28fcf096-6962-498b-9d2e-16d38dc6b7f6: {'id': '28fcf096-6962-498b-9d2e-16d38dc6b7f6', 'status': 'deployed', 'message': 'cse-msr-dev-b7f6: Workspace was deployed successfully...'} It should also be evident from the message flow where the current processing is stuck or failed. Failed deployment status should also be available in the GET /api/workspaces/{workspace_id} and this is just another way to confirm it.","title":"Checking the logs in App Insights"},{"location":"tre-admins/troubleshooting-guide/#checking-the-virtual-machine-scale-setvmss-instance-running-resource-processor","text":"If you see messages hanging in the service bus queue then the resource processor is not up and running. Verify that the VMSS instance is up and healthy. The processor runs in a vnet, and you cannot connect to it directly. If the instance is up then you need to connect to the instance using Bastion. Bastion is already deployed, and you can use the username adminuser and the password is stored in the keyvault under the secret resource-processor-vmss-password Info You cannot see secrets unless you are added to a suitable Access Policy for the keyvault. After logging in you should check the status of cloud-init which is used to bootstrap the machine with docker and start the processor. Log files for cloud init are /var/log/cloud-init.log /var/log/cloud-init-output.log If the docker container is pulled as shown in logs then the resource processor should start. You can check the status of the container using docker ps However, if you see nothing (and the container was pulled) then the processor has either not started yet or it has crashed. You can check all docker processes using docker ps -a which should show if the container terminated before. You can get the logs from container using docker logs <container_id> command. If you want to start a processor container manually you can use the following command and execute from root(/) of filesystem. docker run -v /var/run/docker.sock:/var/run/docker.sock --env-file .env --name resource_processor_vmss_porter_debug runner_image:tag runner_image:tag can be obtained using docker ps Info All logs which you see from the resource processor should also be transferred to the App Insights instance as noted above, so it is not essential to follow the progress by logging into the instance. Logging into the instance and starting a container manually is helpful in live debugging.","title":"Checking the Virtual Machine Scale Set(VMSS) instance running resource processor"},{"location":"tre-admins/troubleshooting-guide/#updating-the-running-container","text":"If you start a container manually you will probably want to install software, for example, an editor. However, firewall blocks all ingress traffic, so you cannot run sudo apt update . You need to add an override rule in the firewall to allow the traffic. Remember to remove this rule when debugging is done.","title":"Updating the running container"},{"location":"tre-admins/deploying-the-tre/auth/","text":"Authentication and authorization This document describes the authentication and authorization (A&A) of deployed Azure TRE system. The backbone of A&A is Azure Active Directory (AAD) . It holds the identities of all TRE/workspace users, including administrators, and connects the identities with app registrations defining the privileges per user roles. App registrations App registrations (represented by service principals) define the privileges enabling access to the TRE system (e.g., API ) as well as the workspaces. It is recommended to run the /scripts/aad-app-reg.sh script to create the two main app registrations: TRE API and TRE Swagger UI . This automatically sets up the app registrations with the required permissions to run Azure TRE. The script will create an app password (client secret) for the TRE API app; make sure to take note of it in the script output as it is only shown once. Alternatively you can also choose to create the app registrations manually via the Azure Portal - see Quickstart: Register an application with the Microsoft identity platform on how. The required setup with permissions is documented below. That is the authentication and authorization setup needed to run the Azure TRE. Below is details about the permissions and if you want to set up the end-to-end automated tests, as it requires a third app registration . Workspaces rely on app registrations as well, and those are documented under Workspaces . TRE API The TRE API app registration defines the permissions, scopes and app roles for API users to authenticate and authorize API calls. API permissions - TRE API API/permission name Type Description Admin consent required Status TRE usage Microsoft Graph/Directory.Read.All ( https://graph.microsoft.com/Directory.Read.All ) Application* Allows the app to read data in your organization's directory, such as users, groups and apps, without a signed-in user. Yes Granted for [directory name] Used e.g., to retrieve app registration details, user associated app roles etc. Microsoft Graph/User.Read.All ( https://graph.microsoft.com/User.Read.All ) Application* Allows the app to read user profiles without a signed in user. Yes Granted for [directory name] Reading user role assignments to check that the user has permissions to execute an action e.g., to view workspaces. See /api_app/services/aad_access_service.py . *) See the difference between delegated and application permission types. See Microsoft Graph permissions reference for more details. Scopes - TRE API api://<Application (client) ID>/ Workspace.Read - Allow the app to get information about the TRE workspaces on behalf of the signed-in user api://<Application (client) ID>/ Workspace.Write - Allow the app to create, update or delete TRE workspaces on behalf of the signed-in user App roles - TRE API Display name Description Allowed member types Value TRE Administrators Provides resource administrator access to the TRE. Users/Groups,Applications TREAdmin TRE Users Provides access to the TRE application. Users/Groups,Applications TREUser Authentication - TRE API The TRE API app registration requires no redirect URLs defined or anything else for that matter. From a security standpoint it should be noted that public client flows should not be allowed (see the image below taken from app registration authentication blade in Azure Portal). TRE Swagger UI TRE Swagger UI app registration: Controls the access to the Swagger UI of the TRE API Has no scopes or app roles defined API permissions - TRE Swagger UI API/permission name Type Description Admin consent required Status Microsoft Graph/offline_access ( https://graph.microsoft.com/offline_access ) Delegated* Allows the app to see and update the data you gave it access to, even when users are not currently using the app. No Granted for [directory name] Microsoft Graph/openid ( https://graph.microsoft.com/openid ) Delegated* Allows users to sign in to the app with their work or school accounts and allows the app to see basic user profile information. No Granted for [directory name] TRE API/Workspace.Read ( api://<TRE API Application (client) ID>/Workspace.Read ) Delegated* See TRE API app registration scopes . No Granted for [directory name] TRE API/Workspace.Write ( api://<TRE API Application (client) ID>/Workspace.Write ) Delegated* See TRE API app registration scopes . No Granted for [directory name] *) See the difference between delegated and application permission types. Authentication - TRE Swagger UI Redirect URLs: https://<app name>.<location>.cloudapp.azure.com/docs/oauth2-redirect http://localhost:8000/docs/oauth2-redirect - For local testing The Swagger UI is a public client, so public client flows need to be enabled: TRE e2e test The TRE e2e test app registration is used to authorize end-to-end test scenarios. It has no scopes or app roles defined. Note This app registration is only needed and used for testing As of writing this, there is no automated way provided for creating the TRE e2e test app registration, so it needs to be created manually. API permissions - TRE e2e test API/permission name Type Description Admin consent required Microsoft Graph/openid ( https://graph.microsoft.com/openid ) Delegated Allows users to sign in to the app with their work or school accounts and allows the app to see basic user profile information. No Microsoft Graph/User.Read ( https://graph.microsoft.com/User.Read ) Delegated Allows users to sign-in to the app, and allows the app to read the profile of signed-in users. It also allows the app to read basic company information of signed-in users. No .Workspace.Read Delegated Allow the app to get information about the TRE workspaces on behalf of the signed-in user No .Workspace.Write Delegated Allow the app to create, update or delete TRE workspaces on behalf of the signed-in user No Authentication - TRE e2e test Define Redirect URLs: In the TRE e2e test app registration go to Authentication -> Add platform -> Select Mobile & Desktop and add: https://login.microsoftonline.com/common/oauth2/nativeclient msal<TRE e2e test app registration application (client) ID>://auth Allow public client flows (see the image below). This enables the end-to-end tests to use a username and password combination to authenticate. Warning Public client flows should never be allowed for a production environment as it poses a security risk. End-to-end test user The end-to-end test authentication and authorization is done via a dummy user, using its username and password, dedicated just for running the tests. The user is linked to the application (app registration) the same way as any other users (see Enabling users ). The end-to-end test should be added to TRE Administrator role exposed by the TRE API application, and to the Owners role exposed by the Workspaces application. Workspaces Access to workspaces is also controlled using app registrations - one per workspace. The configuration of the app registration depends on the nature of the workspace, but this section covers the typical minimum settings. Caution The app registration for a workspace is not created by the API . One needs to be present (created manually) before using the API to provision a new workspace. Authentication - Workspaces Same as TRE API . API permissions - Workspaces API/permission name Type Description Admin consent required Microsoft Graph/User.Read ( https://graph.microsoft.com/User.Read ) Delegated Allows users to sign-in to the app, and allows the app to read the profile of signed-in users. It also allows the app to read basic company information of signed-in users. No App roles Display name Description Allowed member types Value Owners Provides ownership access to workspace. Users/Groups WorkspaceOwner Researchers Provides access to workspace. Users/Groups WorkspaceResearcher Enabling users For a user to gain access to the system, they have to: Have an identity in Azure AD Be linked with an app registration and assigned a role When these requirements are met, the user can sign-in using their credentials and use their privileges to use the API, login to workspace environment etc. based on their specific roles. The users can also be linked via the Enterprise application view:","title":"Authentication and Authorization"},{"location":"tre-admins/deploying-the-tre/auth/#authentication-and-authorization","text":"This document describes the authentication and authorization (A&A) of deployed Azure TRE system. The backbone of A&A is Azure Active Directory (AAD) . It holds the identities of all TRE/workspace users, including administrators, and connects the identities with app registrations defining the privileges per user roles.","title":"Authentication and authorization"},{"location":"tre-admins/deploying-the-tre/auth/#app-registrations","text":"App registrations (represented by service principals) define the privileges enabling access to the TRE system (e.g., API ) as well as the workspaces. It is recommended to run the /scripts/aad-app-reg.sh script to create the two main app registrations: TRE API and TRE Swagger UI . This automatically sets up the app registrations with the required permissions to run Azure TRE. The script will create an app password (client secret) for the TRE API app; make sure to take note of it in the script output as it is only shown once. Alternatively you can also choose to create the app registrations manually via the Azure Portal - see Quickstart: Register an application with the Microsoft identity platform on how. The required setup with permissions is documented below. That is the authentication and authorization setup needed to run the Azure TRE. Below is details about the permissions and if you want to set up the end-to-end automated tests, as it requires a third app registration . Workspaces rely on app registrations as well, and those are documented under Workspaces .","title":"App registrations"},{"location":"tre-admins/deploying-the-tre/auth/#tre-api","text":"The TRE API app registration defines the permissions, scopes and app roles for API users to authenticate and authorize API calls.","title":"TRE API"},{"location":"tre-admins/deploying-the-tre/auth/#api-permissions-tre-api","text":"API/permission name Type Description Admin consent required Status TRE usage Microsoft Graph/Directory.Read.All ( https://graph.microsoft.com/Directory.Read.All ) Application* Allows the app to read data in your organization's directory, such as users, groups and apps, without a signed-in user. Yes Granted for [directory name] Used e.g., to retrieve app registration details, user associated app roles etc. Microsoft Graph/User.Read.All ( https://graph.microsoft.com/User.Read.All ) Application* Allows the app to read user profiles without a signed in user. Yes Granted for [directory name] Reading user role assignments to check that the user has permissions to execute an action e.g., to view workspaces. See /api_app/services/aad_access_service.py . *) See the difference between delegated and application permission types. See Microsoft Graph permissions reference for more details.","title":"API permissions - TRE API"},{"location":"tre-admins/deploying-the-tre/auth/#scopes-tre-api","text":"api://<Application (client) ID>/ Workspace.Read - Allow the app to get information about the TRE workspaces on behalf of the signed-in user api://<Application (client) ID>/ Workspace.Write - Allow the app to create, update or delete TRE workspaces on behalf of the signed-in user","title":"Scopes - TRE API"},{"location":"tre-admins/deploying-the-tre/auth/#app-roles-tre-api","text":"Display name Description Allowed member types Value TRE Administrators Provides resource administrator access to the TRE. Users/Groups,Applications TREAdmin TRE Users Provides access to the TRE application. Users/Groups,Applications TREUser","title":"App roles - TRE API"},{"location":"tre-admins/deploying-the-tre/auth/#authentication-tre-api","text":"The TRE API app registration requires no redirect URLs defined or anything else for that matter. From a security standpoint it should be noted that public client flows should not be allowed (see the image below taken from app registration authentication blade in Azure Portal).","title":"Authentication - TRE API"},{"location":"tre-admins/deploying-the-tre/auth/#tre-swagger-ui","text":"TRE Swagger UI app registration: Controls the access to the Swagger UI of the TRE API Has no scopes or app roles defined","title":"TRE Swagger UI"},{"location":"tre-admins/deploying-the-tre/auth/#api-permissions-tre-swagger-ui","text":"API/permission name Type Description Admin consent required Status Microsoft Graph/offline_access ( https://graph.microsoft.com/offline_access ) Delegated* Allows the app to see and update the data you gave it access to, even when users are not currently using the app. No Granted for [directory name] Microsoft Graph/openid ( https://graph.microsoft.com/openid ) Delegated* Allows users to sign in to the app with their work or school accounts and allows the app to see basic user profile information. No Granted for [directory name] TRE API/Workspace.Read ( api://<TRE API Application (client) ID>/Workspace.Read ) Delegated* See TRE API app registration scopes . No Granted for [directory name] TRE API/Workspace.Write ( api://<TRE API Application (client) ID>/Workspace.Write ) Delegated* See TRE API app registration scopes . No Granted for [directory name] *) See the difference between delegated and application permission types.","title":"API permissions - TRE Swagger UI"},{"location":"tre-admins/deploying-the-tre/auth/#authentication-tre-swagger-ui","text":"Redirect URLs: https://<app name>.<location>.cloudapp.azure.com/docs/oauth2-redirect http://localhost:8000/docs/oauth2-redirect - For local testing The Swagger UI is a public client, so public client flows need to be enabled:","title":"Authentication - TRE Swagger UI"},{"location":"tre-admins/deploying-the-tre/auth/#tre-e2e-test","text":"The TRE e2e test app registration is used to authorize end-to-end test scenarios. It has no scopes or app roles defined. Note This app registration is only needed and used for testing As of writing this, there is no automated way provided for creating the TRE e2e test app registration, so it needs to be created manually.","title":"TRE e2e test"},{"location":"tre-admins/deploying-the-tre/auth/#api-permissions-tre-e2e-test","text":"API/permission name Type Description Admin consent required Microsoft Graph/openid ( https://graph.microsoft.com/openid ) Delegated Allows users to sign in to the app with their work or school accounts and allows the app to see basic user profile information. No Microsoft Graph/User.Read ( https://graph.microsoft.com/User.Read ) Delegated Allows users to sign-in to the app, and allows the app to read the profile of signed-in users. It also allows the app to read basic company information of signed-in users. No .Workspace.Read Delegated Allow the app to get information about the TRE workspaces on behalf of the signed-in user No .Workspace.Write Delegated Allow the app to create, update or delete TRE workspaces on behalf of the signed-in user No","title":"API permissions - TRE e2e test"},{"location":"tre-admins/deploying-the-tre/auth/#authentication-tre-e2e-test","text":"Define Redirect URLs: In the TRE e2e test app registration go to Authentication -> Add platform -> Select Mobile & Desktop and add: https://login.microsoftonline.com/common/oauth2/nativeclient msal<TRE e2e test app registration application (client) ID>://auth Allow public client flows (see the image below). This enables the end-to-end tests to use a username and password combination to authenticate. Warning Public client flows should never be allowed for a production environment as it poses a security risk.","title":"Authentication - TRE e2e test"},{"location":"tre-admins/deploying-the-tre/auth/#end-to-end-test-user","text":"The end-to-end test authentication and authorization is done via a dummy user, using its username and password, dedicated just for running the tests. The user is linked to the application (app registration) the same way as any other users (see Enabling users ). The end-to-end test should be added to TRE Administrator role exposed by the TRE API application, and to the Owners role exposed by the Workspaces application.","title":"End-to-end test user"},{"location":"tre-admins/deploying-the-tre/auth/#workspaces","text":"Access to workspaces is also controlled using app registrations - one per workspace. The configuration of the app registration depends on the nature of the workspace, but this section covers the typical minimum settings. Caution The app registration for a workspace is not created by the API . One needs to be present (created manually) before using the API to provision a new workspace.","title":"Workspaces"},{"location":"tre-admins/deploying-the-tre/auth/#authentication-workspaces","text":"Same as TRE API .","title":"Authentication - Workspaces"},{"location":"tre-admins/deploying-the-tre/auth/#api-permissions-workspaces","text":"API/permission name Type Description Admin consent required Microsoft Graph/User.Read ( https://graph.microsoft.com/User.Read ) Delegated Allows users to sign-in to the app, and allows the app to read the profile of signed-in users. It also allows the app to read basic company information of signed-in users. No","title":"API permissions - Workspaces"},{"location":"tre-admins/deploying-the-tre/auth/#app-roles","text":"Display name Description Allowed member types Value Owners Provides ownership access to workspace. Users/Groups WorkspaceOwner Researchers Provides access to workspace. Users/Groups WorkspaceResearcher","title":"App roles"},{"location":"tre-admins/deploying-the-tre/auth/#enabling-users","text":"For a user to gain access to the system, they have to: Have an identity in Azure AD Be linked with an app registration and assigned a role When these requirements are met, the user can sign-in using their credentials and use their privileges to use the API, login to workspace environment etc. based on their specific roles. The users can also be linked via the Enterprise application view:","title":"Enabling users"},{"location":"tre-admins/deploying-the-tre/bootstrapping/","text":"Bootstrapping This document covers the steps that have to be executed manually before Azure TRE can be deployed using CI/CD. Login to Azure Run login command and select the Azure subscription you wish to deploy Azure TRE to: az login az account list az account set --subscription <subscription ID> Caution When running locally the credentials of the logged-in user will be used to deploy the infrastructure. Hence, it is essential that the user has enough permissions to deploy all resources. See Sign in with Azure CLI for more details. Create service principals A service principal needs to be created to authorize CI/CD workflows to provision resources for the TRE workspaces and workspace services. Create a main service principal with \" Owner \" role: az ad sp create-for-rbac --name \"sp-aztre-core\" --role Owner --scopes /subscriptions/<subscription_id> --sdk-auth Save the JSON output locally - as you will need it later. Create a GitHub secret called AZURE_CREDENTIALS with the JSON output. Create app registrations Create app registrations for auth based on the Authentication & authorization guide.","title":"Bootstrapping"},{"location":"tre-admins/deploying-the-tre/bootstrapping/#bootstrapping","text":"This document covers the steps that have to be executed manually before Azure TRE can be deployed using CI/CD.","title":"Bootstrapping"},{"location":"tre-admins/deploying-the-tre/bootstrapping/#login-to-azure","text":"Run login command and select the Azure subscription you wish to deploy Azure TRE to: az login az account list az account set --subscription <subscription ID> Caution When running locally the credentials of the logged-in user will be used to deploy the infrastructure. Hence, it is essential that the user has enough permissions to deploy all resources. See Sign in with Azure CLI for more details.","title":"Login to Azure"},{"location":"tre-admins/deploying-the-tre/bootstrapping/#create-service-principals","text":"A service principal needs to be created to authorize CI/CD workflows to provision resources for the TRE workspaces and workspace services. Create a main service principal with \" Owner \" role: az ad sp create-for-rbac --name \"sp-aztre-core\" --role Owner --scopes /subscriptions/<subscription_id> --sdk-auth Save the JSON output locally - as you will need it later. Create a GitHub secret called AZURE_CREDENTIALS with the JSON output.","title":"Create service principals"},{"location":"tre-admins/deploying-the-tre/bootstrapping/#create-app-registrations","text":"Create app registrations for auth based on the Authentication & authorization guide.","title":"Create app registrations"},{"location":"tre-admins/deploying-the-tre/manual-deployment/","text":"Deploying Azure TRE manually By following this guide you will deploy a new Azure TRE instance for development and testing purposes. Steps Bootstrap and create prerequisite resources By now you should have a developer environment set up Create app registrations for auth; follow the Authentication & authorization guide Configure variables Before running any of the scripts, the configuration variables need to be set. This is done in an .env file, and this file is read and parsed by the scripts. Info The .tfvars file is not used, this is intentional. The .env file format is easier to parse, meaning we can use the values for bash scripts and other purposes. Copy /devops/.env.sample to /devops/.env . cp devops/.env.sample devops/.env Then, open the .env file in a text editor and set the values for the required variables described in the table below: Environment variable name Description LOCATION The Azure location (region) for all resources. MGMT_RESOURCE_GROUP_NAME The shared resource group for all management resources, including the storage account. MGMT_STORAGE_ACCOUNT_NAME The name of the storage account to hold the Terraform state and other deployment artifacts. TERRAFORM_STATE_CONTAINER_NAME The name of the blob container to hold the Terraform state Default value is tfstate . IMAGE_TAG The default tag for Docker images that will be pushed to the container registry and deployed with the Azure TRE. ACR_NAME A globally unique name for the Azure Container Registry (ACR) that will be created to store deployment images. ARM_SUBSCRIPTION_ID Optional for manual deployment. If not specified the az cli selected subscription will be used. The Azure subscription ID for all resources. ARM_CLIENT_ID Optional for manual deployment without logged-in credentials. The client whose azure identity will be used to deploy the solution. ARM_CLIENT_SECRET Optional for manual deployment without logged-in credentials. The password of the client defined in ARM_CLIENT_ID . ARM_TENANT_ID Optional for manual deployment. If not specified the az cli selected subscription will be used. The AAD tenant of the client defined in ARM_CLIENT_ID . PORTER_OUTPUT_CONTAINER_NAME The name of the storage container where to store the workspace/workspace service deployment output. Workspaces and workspace templates are implemented using Porter bundles - hence the name of the variable. The storage account used is the one defined in STATE_STORAGE_ACCOUNT_NAME . DEBUG If set to \"true\" disables purge protection of keyvault. Copy /templates/core/.env.sample to /templates/core/.env and set values for all variables described in the table below: cp templates/core/.env.sample templates/core/.env Environment variable name Description TRE_ID A globally unique identifier. TRE_ID can be found in the resource names of the Azure TRE instance; for example, a TRE_ID of mytre-dev-3142 will result in a resource group name for Azure TRE instance of rg-mytre-dev-3142 . This must be less than 12 characters. Allowed characters: Alphanumeric, underscores, and hyphens. CORE_ADDRESS_SPACE The address space for the Azure TRE core virtual network. /22 or larger. TRE_ADDRESS_SPACE The address space for the whole TRE environment virtual network where workspaces networks will be created (can include the core network as well). E.g. 10.0.0.0/12 API_IMAGE_TAG The tag of the API image. Make it the same as IMAGE_TAG above. RESOURCE_PROCESSOR_VMSS_PORTER_IMAGE_TAG The tag of the resource processor image. Make it the same as IMAGE_TAG above. GITEA_IMAGE_TAG The tag of the Gitea image. Make it the same as IMAGE_TAG above. SWAGGER_UI_CLIENT_ID Generated when following auth guide. Client ID for swagger client to make requests. AAD_TENANT_ID Generated when following auth guide. Tenant id against which auth is performed. API_CLIENT_ID Generated when following auth guide. Client id of the \"TRE API\". API_CLIENT_SECRET Generated when following auth guide. Client secret of the \"TRE API\". DEPLOY_GITEA If set to false disables deployment of the Gitea shared service. DEPLOY_NEXUS If set to false disables deployment of the Nexus shared service. Deploy The deployment of the Azure TRE is done via Terraform. Run: make all Once the deployment is complete, you will see a few output parameters which are the result of your deployment. app_gateway_name = \"agw-<TRE_ID>\" azure_tre_fqdn = \"<TRE_ID>.<LOCATION>.cloudapp.azure.com\" core_resource_group_name = \"rg-<TRE_ID>\" keyvault_name = \"kv-<TRE_ID>\" log_analytics_name = \"log-<TRE_ID>\" static_web_storage = \"stwebaz<TRE_ID>\" The Azure TRE is initially deployed with an invalid self-signed SSL certificate. This certificate is stored in the deployed Key Vault and can/should be replaced with one valid for the configured domain name. To use a certificate from Let's Encrypt , simply run the command: make letsencrypt Caution There are rate limits with Let's Encrypt, so this should not be run when not needed. Details of deployment and infrastructure The following section is for informational purposes, and the steps don't need to be executed as they are part of make all above. Management Infrastructure We will create management infrastructure in your subscription. This includes resources, such as a storage account and container registry that will enable deployment the Azure TRE. Once the infrastructure is deployed we will build the container images required for deployment. The management infrastructure can serve multiple Azure TRE deployments. Bootstrap the back-end state As a principle, we want all the Azure TRE resources defined in Terraform, including the storage account used by Terraform to hold its back-end state. A bootstrap script is used to create the initial storage account and resource group using the Azure CLI. Then Terraform is initialized using this storage account as a back-end, and the storage account imported into the state. You can do this step using the following command but as stated above this is already part of make all . make bootstrap This script should never need running a second time even if the other management resources are modified. Management Resource Deployment The deployment of the rest of the shared management resources is done via Terraform, and the various .tf files in the root of this repo. make mgmt-deploy This Terraform creates & configures the following: Resource Group (also in bootstrap). Storage Account for holding Terraform state (also in bootstrap). Azure Container Registry. Build and push Docker images Build and push the docker images required by the Azure TRE and publish them to the container registry created in the previous step. make build-api-image make build-resource-processor-vm-porter-image make push-api-image make push-resource-processor-vm-porter-image Access the Azure TRE deployment To get the Azure TRE URL, view azure_tre_fqdn in the output of the previous make all command, or run the following command to see it again: cd templates/core/terraform terraform output azure_tre_fqdn Open the following URL in a browser, and you should see the Open API docs of Azure TRE API. https://<azure_tre_fqdn>/docs You can also create a request to the api/health endpoint to verify that the API is deployed and responds. You should see a pong response as a result of below request. curl https://<azure_tre_fqdn>/api/health Publishing and registering the base workspace bundle Run: register-bundle DIR=./templates/workspaces/base Copy the resulting payload json. Navigate to the Swagger UI at https://<azure_tre_fqdn>/docs Log into the Swagger UI by clicking Authorize , then Authorize again. You will be redirected to the login page. Once logged in. Click Try it out on the POST /api/workspace-templates operation: Paste the payload json generated earlier into the Request body field, then click Execute . Review the server response. To verify registration of the template do GET operation on /api/workspace-templates . The name of the template should now be listed. Creating a base workspace Now that we have published and registered a base workspace bundle we can use the deployed API to create a base workspace. Info All routes are auth protected. Click the green Authorize button to receive a token for swagger client. As explained in the auth guide , every workspace has a corresponding app registration which can be created using the helper script /scripts/workspace-app-reg.py . Multiple workspaces can share an app registration. Running the script will report app id of the generated app which needs to be used in the POST body below. Go to azure_tre_fqdn/docs and use POST /api/workspaces with the sample body to create a base workspace. { \"displayName\" : \"manual-from-swagger\" , \"description\" : \"workspace for team X\" , \"workspaceType\" : \"tre-workspace-base\" , \"parameters\" : {}, \"authConfig\" : { \"provider\" : \"AAD\" , \"data\" : { \"app_id\" : \"app id created above\" } } } The API will report the workspace_id of the created workspace, which can be used to query deployment status by using /api/workspaces/<workspace_id> You can also follow the progress in Azure portal as various resources come up. Info To query the status using the API your user needs to have TREResearcher or TREOwner role assigned to the app. Deleting the Azure TRE deployment To remove the Azure TRE and its resources from your Azure subscription run: make tre-destroy","title":"Manual Deployment"},{"location":"tre-admins/deploying-the-tre/manual-deployment/#deploying-azure-tre-manually","text":"By following this guide you will deploy a new Azure TRE instance for development and testing purposes.","title":"Deploying Azure TRE manually"},{"location":"tre-admins/deploying-the-tre/manual-deployment/#steps","text":"","title":"Steps"},{"location":"tre-admins/deploying-the-tre/manual-deployment/#bootstrap-and-create-prerequisite-resources","text":"By now you should have a developer environment set up Create app registrations for auth; follow the Authentication & authorization guide","title":"Bootstrap and create prerequisite resources"},{"location":"tre-admins/deploying-the-tre/manual-deployment/#configure-variables","text":"Before running any of the scripts, the configuration variables need to be set. This is done in an .env file, and this file is read and parsed by the scripts. Info The .tfvars file is not used, this is intentional. The .env file format is easier to parse, meaning we can use the values for bash scripts and other purposes. Copy /devops/.env.sample to /devops/.env . cp devops/.env.sample devops/.env Then, open the .env file in a text editor and set the values for the required variables described in the table below: Environment variable name Description LOCATION The Azure location (region) for all resources. MGMT_RESOURCE_GROUP_NAME The shared resource group for all management resources, including the storage account. MGMT_STORAGE_ACCOUNT_NAME The name of the storage account to hold the Terraform state and other deployment artifacts. TERRAFORM_STATE_CONTAINER_NAME The name of the blob container to hold the Terraform state Default value is tfstate . IMAGE_TAG The default tag for Docker images that will be pushed to the container registry and deployed with the Azure TRE. ACR_NAME A globally unique name for the Azure Container Registry (ACR) that will be created to store deployment images. ARM_SUBSCRIPTION_ID Optional for manual deployment. If not specified the az cli selected subscription will be used. The Azure subscription ID for all resources. ARM_CLIENT_ID Optional for manual deployment without logged-in credentials. The client whose azure identity will be used to deploy the solution. ARM_CLIENT_SECRET Optional for manual deployment without logged-in credentials. The password of the client defined in ARM_CLIENT_ID . ARM_TENANT_ID Optional for manual deployment. If not specified the az cli selected subscription will be used. The AAD tenant of the client defined in ARM_CLIENT_ID . PORTER_OUTPUT_CONTAINER_NAME The name of the storage container where to store the workspace/workspace service deployment output. Workspaces and workspace templates are implemented using Porter bundles - hence the name of the variable. The storage account used is the one defined in STATE_STORAGE_ACCOUNT_NAME . DEBUG If set to \"true\" disables purge protection of keyvault. Copy /templates/core/.env.sample to /templates/core/.env and set values for all variables described in the table below: cp templates/core/.env.sample templates/core/.env Environment variable name Description TRE_ID A globally unique identifier. TRE_ID can be found in the resource names of the Azure TRE instance; for example, a TRE_ID of mytre-dev-3142 will result in a resource group name for Azure TRE instance of rg-mytre-dev-3142 . This must be less than 12 characters. Allowed characters: Alphanumeric, underscores, and hyphens. CORE_ADDRESS_SPACE The address space for the Azure TRE core virtual network. /22 or larger. TRE_ADDRESS_SPACE The address space for the whole TRE environment virtual network where workspaces networks will be created (can include the core network as well). E.g. 10.0.0.0/12 API_IMAGE_TAG The tag of the API image. Make it the same as IMAGE_TAG above. RESOURCE_PROCESSOR_VMSS_PORTER_IMAGE_TAG The tag of the resource processor image. Make it the same as IMAGE_TAG above. GITEA_IMAGE_TAG The tag of the Gitea image. Make it the same as IMAGE_TAG above. SWAGGER_UI_CLIENT_ID Generated when following auth guide. Client ID for swagger client to make requests. AAD_TENANT_ID Generated when following auth guide. Tenant id against which auth is performed. API_CLIENT_ID Generated when following auth guide. Client id of the \"TRE API\". API_CLIENT_SECRET Generated when following auth guide. Client secret of the \"TRE API\". DEPLOY_GITEA If set to false disables deployment of the Gitea shared service. DEPLOY_NEXUS If set to false disables deployment of the Nexus shared service.","title":"Configure variables"},{"location":"tre-admins/deploying-the-tre/manual-deployment/#deploy","text":"The deployment of the Azure TRE is done via Terraform. Run: make all Once the deployment is complete, you will see a few output parameters which are the result of your deployment. app_gateway_name = \"agw-<TRE_ID>\" azure_tre_fqdn = \"<TRE_ID>.<LOCATION>.cloudapp.azure.com\" core_resource_group_name = \"rg-<TRE_ID>\" keyvault_name = \"kv-<TRE_ID>\" log_analytics_name = \"log-<TRE_ID>\" static_web_storage = \"stwebaz<TRE_ID>\" The Azure TRE is initially deployed with an invalid self-signed SSL certificate. This certificate is stored in the deployed Key Vault and can/should be replaced with one valid for the configured domain name. To use a certificate from Let's Encrypt , simply run the command: make letsencrypt Caution There are rate limits with Let's Encrypt, so this should not be run when not needed.","title":"Deploy"},{"location":"tre-admins/deploying-the-tre/manual-deployment/#details-of-deployment-and-infrastructure","text":"The following section is for informational purposes, and the steps don't need to be executed as they are part of make all above.","title":"Details of deployment and infrastructure"},{"location":"tre-admins/deploying-the-tre/manual-deployment/#management-infrastructure","text":"We will create management infrastructure in your subscription. This includes resources, such as a storage account and container registry that will enable deployment the Azure TRE. Once the infrastructure is deployed we will build the container images required for deployment. The management infrastructure can serve multiple Azure TRE deployments.","title":"Management Infrastructure"},{"location":"tre-admins/deploying-the-tre/manual-deployment/#bootstrap-the-back-end-state","text":"As a principle, we want all the Azure TRE resources defined in Terraform, including the storage account used by Terraform to hold its back-end state. A bootstrap script is used to create the initial storage account and resource group using the Azure CLI. Then Terraform is initialized using this storage account as a back-end, and the storage account imported into the state. You can do this step using the following command but as stated above this is already part of make all . make bootstrap This script should never need running a second time even if the other management resources are modified.","title":"Bootstrap the back-end state"},{"location":"tre-admins/deploying-the-tre/manual-deployment/#management-resource-deployment","text":"The deployment of the rest of the shared management resources is done via Terraform, and the various .tf files in the root of this repo. make mgmt-deploy This Terraform creates & configures the following: Resource Group (also in bootstrap). Storage Account for holding Terraform state (also in bootstrap). Azure Container Registry.","title":"Management Resource Deployment"},{"location":"tre-admins/deploying-the-tre/manual-deployment/#build-and-push-docker-images","text":"Build and push the docker images required by the Azure TRE and publish them to the container registry created in the previous step. make build-api-image make build-resource-processor-vm-porter-image make push-api-image make push-resource-processor-vm-porter-image","title":"Build and push Docker images"},{"location":"tre-admins/deploying-the-tre/manual-deployment/#access-the-azure-tre-deployment","text":"To get the Azure TRE URL, view azure_tre_fqdn in the output of the previous make all command, or run the following command to see it again: cd templates/core/terraform terraform output azure_tre_fqdn Open the following URL in a browser, and you should see the Open API docs of Azure TRE API. https://<azure_tre_fqdn>/docs You can also create a request to the api/health endpoint to verify that the API is deployed and responds. You should see a pong response as a result of below request. curl https://<azure_tre_fqdn>/api/health","title":"Access the Azure TRE deployment"},{"location":"tre-admins/deploying-the-tre/manual-deployment/#publishing-and-registering-the-base-workspace-bundle","text":"Run: register-bundle DIR=./templates/workspaces/base Copy the resulting payload json. Navigate to the Swagger UI at https://<azure_tre_fqdn>/docs Log into the Swagger UI by clicking Authorize , then Authorize again. You will be redirected to the login page. Once logged in. Click Try it out on the POST /api/workspace-templates operation: Paste the payload json generated earlier into the Request body field, then click Execute . Review the server response. To verify registration of the template do GET operation on /api/workspace-templates . The name of the template should now be listed.","title":"Publishing and registering the base workspace bundle"},{"location":"tre-admins/deploying-the-tre/manual-deployment/#creating-a-base-workspace","text":"Now that we have published and registered a base workspace bundle we can use the deployed API to create a base workspace. Info All routes are auth protected. Click the green Authorize button to receive a token for swagger client. As explained in the auth guide , every workspace has a corresponding app registration which can be created using the helper script /scripts/workspace-app-reg.py . Multiple workspaces can share an app registration. Running the script will report app id of the generated app which needs to be used in the POST body below. Go to azure_tre_fqdn/docs and use POST /api/workspaces with the sample body to create a base workspace. { \"displayName\" : \"manual-from-swagger\" , \"description\" : \"workspace for team X\" , \"workspaceType\" : \"tre-workspace-base\" , \"parameters\" : {}, \"authConfig\" : { \"provider\" : \"AAD\" , \"data\" : { \"app_id\" : \"app id created above\" } } } The API will report the workspace_id of the created workspace, which can be used to query deployment status by using /api/workspaces/<workspace_id> You can also follow the progress in Azure portal as various resources come up. Info To query the status using the API your user needs to have TREResearcher or TREOwner role assigned to the app.","title":"Creating a base workspace"},{"location":"tre-admins/deploying-the-tre/manual-deployment/#deleting-the-azure-tre-deployment","text":"To remove the Azure TRE and its resources from your Azure subscription run: make tre-destroy","title":"Deleting the Azure TRE deployment"},{"location":"tre-admins/deploying-the-tre/workflows/","text":"GitHub Actions workflows (CI/CD) Setup instructions These are onetime configuration steps required to set up the GitHub Actions workflows (pipelines). After the steps the TRE deployment workflow ( /.github/workflows/deploy_tre.yml ) is ready to run. Create service principal and set their repository secrets as explained in Bootstrapping Create app registrations for auth based on the Authentication & authorization guide Set other repository secrets as explained in the table below Required repository secrets for the CI/CD. Secret name Description AZURE_CREDENTIALS Explained in Bootstrapping - Create service principals . Main service principal credentials output. TF_STATE_CONTAINER The name of the blob container to hold the Terraform state. By convention the value is tfstate . MGMT_RESOURCE_GROUP The name of the shared resource group for all Azure TRE core resources. STATE_STORAGE_ACCOUNT_NAME The name of the storage account to hold the Terraform state and other deployment artifacts. E.g. mystorageaccount . LOCATION The Azure location (region) for all resources. E.g. westeurope ACR_NAME A globally unique name for the Azure Container Registry (ACR) that will be created to store deployment images. PORTER_OUTPUT_CONTAINER_NAME The name of the storage container where to store the workspace/workspace service deployment output. Workspaces and workspace templates are implemented using Porter bundles - hence the name of the secret. The storage account used is the same as defined by STATE_STORAGE_ACCOUNT_NAME . TRE_ID A globally unique identifier. TRE_ID can be found in the resource names of the Azure TRE instance; for example, a TRE_ID of tre-dev-42 will result in a resource group name for Azure TRE instance of rg-tre-dev-42 . This must be less than 12 characters. Allowed characters: Alphanumeric, underscores, and hyphens. CORE_ADDRESS_SPACE The address space for the Azure TRE core virtual network. E.g. 10.1.0.0/22 . Recommended /22 or larger. TRE_ADDRESS_SPACE The address space for the whole TRE environment virtual network where workspaces networks will be created (can include the core network as well). E.g. 10.0.0.0/12 SWAGGER_UI_CLIENT_ID The application (client) ID of the TRE Swagger UI service principal. AAD_TENANT_ID The tenant ID of the Azure AD. API_CLIENT_ID The application (client) ID of the TRE API service principal. API_CLIENT_SECRET The application password (client secret) of the TRE API service principal. DEPLOY_GITEA If set to false disables deployment of the Gitea shared service. DEPLOY_NEXUS If set to false disables deployment of the Nexus shared service. TEST_APP_ID The application (client) ID of the E2E Test app service principal. TEST_USER_NAME The username of the E2E Test User . TEST_USER_PASSWORD The password of the E2E Test User . TEST_WORKSPACE_APP_ID The application (client) ID of the Workspaces app service principal.","title":"Workflow Deployment"},{"location":"tre-admins/deploying-the-tre/workflows/#github-actions-workflows-cicd","text":"","title":"GitHub Actions workflows (CI/CD)"},{"location":"tre-admins/deploying-the-tre/workflows/#setup-instructions","text":"These are onetime configuration steps required to set up the GitHub Actions workflows (pipelines). After the steps the TRE deployment workflow ( /.github/workflows/deploy_tre.yml ) is ready to run. Create service principal and set their repository secrets as explained in Bootstrapping Create app registrations for auth based on the Authentication & authorization guide Set other repository secrets as explained in the table below Required repository secrets for the CI/CD. Secret name Description AZURE_CREDENTIALS Explained in Bootstrapping - Create service principals . Main service principal credentials output. TF_STATE_CONTAINER The name of the blob container to hold the Terraform state. By convention the value is tfstate . MGMT_RESOURCE_GROUP The name of the shared resource group for all Azure TRE core resources. STATE_STORAGE_ACCOUNT_NAME The name of the storage account to hold the Terraform state and other deployment artifacts. E.g. mystorageaccount . LOCATION The Azure location (region) for all resources. E.g. westeurope ACR_NAME A globally unique name for the Azure Container Registry (ACR) that will be created to store deployment images. PORTER_OUTPUT_CONTAINER_NAME The name of the storage container where to store the workspace/workspace service deployment output. Workspaces and workspace templates are implemented using Porter bundles - hence the name of the secret. The storage account used is the same as defined by STATE_STORAGE_ACCOUNT_NAME . TRE_ID A globally unique identifier. TRE_ID can be found in the resource names of the Azure TRE instance; for example, a TRE_ID of tre-dev-42 will result in a resource group name for Azure TRE instance of rg-tre-dev-42 . This must be less than 12 characters. Allowed characters: Alphanumeric, underscores, and hyphens. CORE_ADDRESS_SPACE The address space for the Azure TRE core virtual network. E.g. 10.1.0.0/22 . Recommended /22 or larger. TRE_ADDRESS_SPACE The address space for the whole TRE environment virtual network where workspaces networks will be created (can include the core network as well). E.g. 10.0.0.0/12 SWAGGER_UI_CLIENT_ID The application (client) ID of the TRE Swagger UI service principal. AAD_TENANT_ID The tenant ID of the Azure AD. API_CLIENT_ID The application (client) ID of the TRE API service principal. API_CLIENT_SECRET The application password (client secret) of the TRE API service principal. DEPLOY_GITEA If set to false disables deployment of the Gitea shared service. DEPLOY_NEXUS If set to false disables deployment of the Nexus shared service. TEST_APP_ID The application (client) ID of the E2E Test app service principal. TEST_USER_NAME The username of the E2E Test User . TEST_USER_PASSWORD The password of the E2E Test User . TEST_WORKSPACE_APP_ID The application (client) ID of the Workspaces app service principal.","title":"Setup instructions"},{"location":"tre-developers/api/","text":"TRE API The TRE API is a service that users can interact with to request changes to workspaces e.g., to create, update, delete workspaces and workspace services inside each workspace. Prerequisites Tools Python >= 3.8 with pip Azure resources Application Insights - Not required for testing locally Azure Cosmos DB (SQL) You can use the Cosmos DB Emulator for testing locally Azure Service Bus Service principal for the API to access Azure services such as Azure Service Bus AAD applications (for the API and Swagger UI) - see Authentication & authorization for more information Creating resources (Bash) The following snippets can be used to create resources for local testing with Bash shell. Sign in with Azure CLI: az login az account list az account set --subscription <subscription ID> Azure Service Bus: RESOURCE_GROUP = <resource group name> LOCATION = westeurope SERVICE_BUS_NAMESPACE = <service bus namespace name> SERVICE_BUS_RESOURCE_REQUEST_QUEUE = workspacequeue SERVICE_BUS_DEPLOYMENT_STATUS_UPDATE_QUEUE = deploymentstatus az servicebus namespace create --resource-group $RESOURCE_GROUP --name $SERVICE_BUS_NAMESPACE --location $LOCATION az servicebus queue create --resource-group $RESOURCE_GROUP --namespace-name $SERVICE_BUS_NAMESPACE --name $SERVICE_BUS_RESOURCE_REQUEST_QUEUE az servicebus queue create --resource-group $RESOURCE_GROUP --namespace-name $SERVICE_BUS_NAMESPACE --name $SERVICE_BUS_DEPLOYMENT_STATUS_UPDATE_QUEUE Azure Cosmos DB: COSMOS_NAME = <cosmos_name> COSMOS_DB_NAME = <database_name> az cosmosdb create -n $COSMOS_NAME -g $RESOURCE_GROUP --locations regionName = $LOCATION az cosmosdb sql database create -a $COSMOS_NAME -g $RESOURCE_GROUP -n $COSMOS_DB_NAME Create a service principal and assign it permissions to access Service Bus: az ad sp create-for-rbac --name <service principal name> SERVICE_PRINCIPAL_ID = <the AppId of the Service Principal> SUBSCRIPTION_ID = $( az account show --query id --output tsv ) az role assignment create \\ --role \"Azure Service Bus Data Sender\" \\ --assignee $SERVICE_PRINCIPAL_ID \\ --scope /subscriptions/ $SUBSCRIPTION_ID /resourceGroups/ $RESOURCE_GROUP /providers/Microsoft.ServiceBus/namespaces/ $SERVICE_BUS_NAMESPACE az role assignment create \\ --role \"Azure Service Bus Data Receiver\" \\ --assignee $SERVICE_PRINCIPAL_ID \\ --scope /subscriptions/ $SUBSCRIPTION_ID /resourceGroups/ $RESOURCE_GROUP /providers/Microsoft.ServiceBus/namespaces/ $SERVICE_BUS_NAMESPACE Caution Keep in mind that Azure role assignments may take up to five minutes to propagate. Configuration General Environment variable name Description DEBUG When set to True , the debugging information for unhandled exceptions is shown in the Swagger UI and logging is more verbose. TRE_ID The Azure TRE instance name - used for deployment of resources (can be set to anything when debugging locally). Example value: mytre-dev-3142 RESOURCE_LOCATION The location (region) to deploy resources (e.g., workspaces) to. This can be set to anything as the deployment service is not called locally. Example value: westeurope Authentication and Authorization The TRE API depends on TRE API and TRE Swagger UI app registrations. The API requires the environment variables listed in the table below to be present. See Authentication and authorization for more information. Environment variable name Description AAD_TENANT_ID The tenant ID of the Azure AD. API_CLIENT_ID The application (client) ID of the TRE API service principal. API_CLIENT_SECRET The application password (client secret) of the TRE API service principal. SWAGGER_UI_CLIENT_ID The application (client) ID of the TRE Swagger UI service principal. See also: Auth in code State store Environment variable name Description STATE_STORE_ENDPOINT The Cosmos DB endpoint. Use localhost with an emulator. Example value: https://localhost:8081 STATE_STORE_KEY The Cosmos DB key. Use only with localhost emulator. COSMOSDB_ACCOUNT_NAME The Cosmos DB account name. SUBSCRIPTION_ID The Azure Subscription ID where Cosmos DB is located. RESOURCE_GROUP_NAME The Azure Resource Group name where Cosmos DB is located. Service Bus Environment variable name Description SERVICE_BUS_FULLY_QUALIFIED_NAMESPACE Example value: <your namespace>.servicebus.windows.net SERVICE_BUS_RESOURCE_REQUEST_QUEUE The queue for resource request messages sent by the API. Example value: workspacequeue SERVICE_BUS_DEPLOYMENT_STATUS_UPDATE_QUEUE The queue for deployment status update messages sent by Resource Processor and received by the API. Example value: deploymentstatus Logging and monitoring Environment variable name Description APPLICATIONINSIGHTS_CONNECTION_STRING Application Insights connection string - can be left blank when debugging locally. APPINSIGHTS_INSTRUMENTATIONKEY Application Insights instrumentation key - can be left blank when debugging locally. Service principal for API process identity Environment variable name Description AZURE_SUBSCRIPTION_ID AZURE_TENANT_ID AZURE_CLIENT_ID AZURE_CLIENT_SECRET Running the API Develop and run locally Install python dependencies (in a virtual environment) virtualenv venv venv/Scripts/activate pip install -r requirements.txt Copy .env.tmpl in the api_app folder to .env and configure the variables. Notice: You might also need to export those variables to your env ( export VAR_NAME=VALUE for all vars in the .env file). Start the web API cd api_app uvicorn main:app --reload The API endpoints documentation and the Swagger UI will be available at https://localhost:8000/docs . Develop and run in a dev container Open the project in Visual Studio Code in the DevContainer Copy .env.sample in the api_app folder to .env and configure the variables Start the web API cd api_app pip install -r requirements.txt pip install -r requirements-dev.txt uvicorn main:app --reload The API endpoints documentation and the Swagger UI will be available at https://localhost:8000/docs . Deploy with Docker You must have Docker and Docker Compose tools installed, and an Azure Cosmos DB configured in .env as described above. Then run: cd api_app docker compose up -d app The API will be available at https://localhost:8000/api in your browser. Unit tests The unit tests are written with pytest and located in folder /api_app/tests_ma/ . Run all unit tests with the following command in the root folder of the repository: pytest --ignore=e2e_tests API application folder structure api_app \u251c\u2500\u2500 api - API implementation \u2502 \u251c\u2500\u2500 dependencies - Dependencies for routes definition \u2502 \u251c\u2500\u2500 errors - Definitions of error handlers \u2502 \u2514\u2500\u2500 routes - Web routes (API endpoints) \u2502 \u251c\u2500\u2500 core - Application configuration, startup events, logging \u2502 \u251c\u2500\u2500 db - Database related implementation \u2502 \u251c\u2500\u2500 migrations - Manually written alembic migrations \u2502 \u2514\u2500\u2500 repositories - All CRUD features \u2502 \u251c\u2500\u2500 models - Pydantic models for this application \u2502 \u251c\u2500\u2500 domain - Main models that are used almost everywhere \u2502 \u2514\u2500\u2500 schemas - Schemas for using in web routes \u2502 \u251c\u2500\u2500 resources - Strings that are used e.g., in web responses \u2502 \u251c\u2500\u2500 services - Logic that is not only CRUD related \u2502 \u251c\u2500\u2500 tests_ma - Unit tests \u2502 \u2514\u2500\u2500 main.py - FastAPI application creation and configuration Auth in code The bulk of the authentication and authorization (A&A) related code of the API is located in /api_app/services/ folder. The A&A code has an abstract base for enabling the possibility to add additional A&A service providers. The Azure Active Directory (AAD) specific implementation is derived as follows: AccessService (access_service.py) <\u2500\u2500\u2500 AADAccessService (aad_access_service.py) fastapi.security.OAuth2AuthorizationCodeBearer <\u2500\u2500\u2500 AzureADAuthorization (aad_authentication.py) All the sensitive routes (API calls that can query sensitive data or modify resources) in the TRE API depend on having a \"current user\" authenticated. E.g., in /api_app/api/routes/workspaces.py : router = APIRouter ( dependencies = [ Depends ( get_current_user )]) Where APIRouter is part of the FastAPI . The user details, once authenticated, are stored as an instance of the custom User class. Auth in workspace requests Some workspace routes require authConfig field in the request body. The AAD specific implementation expects a dictionary inside data field to contain the application (client) ID of the app registration associated with workspace : { \"authConfig\" : { \"provider\" : \"AAD\" , \"data\" : { \"app_id\" : \"c36f2ee3-8ec3-4431-9240-b1c0a0eb80a0\" } } } Caution The app registration for a workspace is not created by the API. One needs to be present (created manually) before using the API to provision a new workspace. Network requirements To be able to run the TRE API it needs to access the following resource outside the Azure TRE VNET via explicit allowed Service Tags or URLs. Service Tag / Destination Justification AzureActiveDirectory Authenticate with the User Assigned identity to access Azure Cosmos DB and Azure Service Bus. AzureMonitor Publish traces and logs to one central place for troubleshooting. AzureResourceManager To perform control plane operations, such as create database in State Store. AzureContainerRegistry Pull the TRE API container image, as it is located in Azure Container Registry. graph.microsoft.com Lookup role assignments for Azure Active Directory user, to only show TRE resources and user has access to.","title":"API"},{"location":"tre-developers/api/#tre-api","text":"The TRE API is a service that users can interact with to request changes to workspaces e.g., to create, update, delete workspaces and workspace services inside each workspace.","title":"TRE API"},{"location":"tre-developers/api/#prerequisites","text":"","title":"Prerequisites"},{"location":"tre-developers/api/#tools","text":"Python >= 3.8 with pip","title":"Tools"},{"location":"tre-developers/api/#azure-resources","text":"Application Insights - Not required for testing locally Azure Cosmos DB (SQL) You can use the Cosmos DB Emulator for testing locally Azure Service Bus Service principal for the API to access Azure services such as Azure Service Bus AAD applications (for the API and Swagger UI) - see Authentication & authorization for more information","title":"Azure resources"},{"location":"tre-developers/api/#creating-resources-bash","text":"The following snippets can be used to create resources for local testing with Bash shell. Sign in with Azure CLI: az login az account list az account set --subscription <subscription ID> Azure Service Bus: RESOURCE_GROUP = <resource group name> LOCATION = westeurope SERVICE_BUS_NAMESPACE = <service bus namespace name> SERVICE_BUS_RESOURCE_REQUEST_QUEUE = workspacequeue SERVICE_BUS_DEPLOYMENT_STATUS_UPDATE_QUEUE = deploymentstatus az servicebus namespace create --resource-group $RESOURCE_GROUP --name $SERVICE_BUS_NAMESPACE --location $LOCATION az servicebus queue create --resource-group $RESOURCE_GROUP --namespace-name $SERVICE_BUS_NAMESPACE --name $SERVICE_BUS_RESOURCE_REQUEST_QUEUE az servicebus queue create --resource-group $RESOURCE_GROUP --namespace-name $SERVICE_BUS_NAMESPACE --name $SERVICE_BUS_DEPLOYMENT_STATUS_UPDATE_QUEUE Azure Cosmos DB: COSMOS_NAME = <cosmos_name> COSMOS_DB_NAME = <database_name> az cosmosdb create -n $COSMOS_NAME -g $RESOURCE_GROUP --locations regionName = $LOCATION az cosmosdb sql database create -a $COSMOS_NAME -g $RESOURCE_GROUP -n $COSMOS_DB_NAME Create a service principal and assign it permissions to access Service Bus: az ad sp create-for-rbac --name <service principal name> SERVICE_PRINCIPAL_ID = <the AppId of the Service Principal> SUBSCRIPTION_ID = $( az account show --query id --output tsv ) az role assignment create \\ --role \"Azure Service Bus Data Sender\" \\ --assignee $SERVICE_PRINCIPAL_ID \\ --scope /subscriptions/ $SUBSCRIPTION_ID /resourceGroups/ $RESOURCE_GROUP /providers/Microsoft.ServiceBus/namespaces/ $SERVICE_BUS_NAMESPACE az role assignment create \\ --role \"Azure Service Bus Data Receiver\" \\ --assignee $SERVICE_PRINCIPAL_ID \\ --scope /subscriptions/ $SUBSCRIPTION_ID /resourceGroups/ $RESOURCE_GROUP /providers/Microsoft.ServiceBus/namespaces/ $SERVICE_BUS_NAMESPACE Caution Keep in mind that Azure role assignments may take up to five minutes to propagate.","title":"Creating resources (Bash)"},{"location":"tre-developers/api/#configuration","text":"","title":"Configuration"},{"location":"tre-developers/api/#general","text":"Environment variable name Description DEBUG When set to True , the debugging information for unhandled exceptions is shown in the Swagger UI and logging is more verbose. TRE_ID The Azure TRE instance name - used for deployment of resources (can be set to anything when debugging locally). Example value: mytre-dev-3142 RESOURCE_LOCATION The location (region) to deploy resources (e.g., workspaces) to. This can be set to anything as the deployment service is not called locally. Example value: westeurope","title":"General"},{"location":"tre-developers/api/#authentication-and-authorization","text":"The TRE API depends on TRE API and TRE Swagger UI app registrations. The API requires the environment variables listed in the table below to be present. See Authentication and authorization for more information. Environment variable name Description AAD_TENANT_ID The tenant ID of the Azure AD. API_CLIENT_ID The application (client) ID of the TRE API service principal. API_CLIENT_SECRET The application password (client secret) of the TRE API service principal. SWAGGER_UI_CLIENT_ID The application (client) ID of the TRE Swagger UI service principal. See also: Auth in code","title":"Authentication and Authorization"},{"location":"tre-developers/api/#state-store","text":"Environment variable name Description STATE_STORE_ENDPOINT The Cosmos DB endpoint. Use localhost with an emulator. Example value: https://localhost:8081 STATE_STORE_KEY The Cosmos DB key. Use only with localhost emulator. COSMOSDB_ACCOUNT_NAME The Cosmos DB account name. SUBSCRIPTION_ID The Azure Subscription ID where Cosmos DB is located. RESOURCE_GROUP_NAME The Azure Resource Group name where Cosmos DB is located.","title":"State store"},{"location":"tre-developers/api/#service-bus","text":"Environment variable name Description SERVICE_BUS_FULLY_QUALIFIED_NAMESPACE Example value: <your namespace>.servicebus.windows.net SERVICE_BUS_RESOURCE_REQUEST_QUEUE The queue for resource request messages sent by the API. Example value: workspacequeue SERVICE_BUS_DEPLOYMENT_STATUS_UPDATE_QUEUE The queue for deployment status update messages sent by Resource Processor and received by the API. Example value: deploymentstatus","title":"Service Bus"},{"location":"tre-developers/api/#logging-and-monitoring","text":"Environment variable name Description APPLICATIONINSIGHTS_CONNECTION_STRING Application Insights connection string - can be left blank when debugging locally. APPINSIGHTS_INSTRUMENTATIONKEY Application Insights instrumentation key - can be left blank when debugging locally.","title":"Logging and monitoring"},{"location":"tre-developers/api/#service-principal-for-api-process-identity","text":"Environment variable name Description AZURE_SUBSCRIPTION_ID AZURE_TENANT_ID AZURE_CLIENT_ID AZURE_CLIENT_SECRET","title":"Service principal for API process identity"},{"location":"tre-developers/api/#running-the-api","text":"","title":"Running the API"},{"location":"tre-developers/api/#develop-and-run-locally","text":"Install python dependencies (in a virtual environment) virtualenv venv venv/Scripts/activate pip install -r requirements.txt Copy .env.tmpl in the api_app folder to .env and configure the variables. Notice: You might also need to export those variables to your env ( export VAR_NAME=VALUE for all vars in the .env file). Start the web API cd api_app uvicorn main:app --reload The API endpoints documentation and the Swagger UI will be available at https://localhost:8000/docs .","title":"Develop and run locally"},{"location":"tre-developers/api/#develop-and-run-in-a-dev-container","text":"Open the project in Visual Studio Code in the DevContainer Copy .env.sample in the api_app folder to .env and configure the variables Start the web API cd api_app pip install -r requirements.txt pip install -r requirements-dev.txt uvicorn main:app --reload The API endpoints documentation and the Swagger UI will be available at https://localhost:8000/docs .","title":"Develop and run in a dev container"},{"location":"tre-developers/api/#deploy-with-docker","text":"You must have Docker and Docker Compose tools installed, and an Azure Cosmos DB configured in .env as described above. Then run: cd api_app docker compose up -d app The API will be available at https://localhost:8000/api in your browser.","title":"Deploy with Docker"},{"location":"tre-developers/api/#unit-tests","text":"The unit tests are written with pytest and located in folder /api_app/tests_ma/ . Run all unit tests with the following command in the root folder of the repository: pytest --ignore=e2e_tests","title":"Unit tests"},{"location":"tre-developers/api/#api-application-folder-structure","text":"api_app \u251c\u2500\u2500 api - API implementation \u2502 \u251c\u2500\u2500 dependencies - Dependencies for routes definition \u2502 \u251c\u2500\u2500 errors - Definitions of error handlers \u2502 \u2514\u2500\u2500 routes - Web routes (API endpoints) \u2502 \u251c\u2500\u2500 core - Application configuration, startup events, logging \u2502 \u251c\u2500\u2500 db - Database related implementation \u2502 \u251c\u2500\u2500 migrations - Manually written alembic migrations \u2502 \u2514\u2500\u2500 repositories - All CRUD features \u2502 \u251c\u2500\u2500 models - Pydantic models for this application \u2502 \u251c\u2500\u2500 domain - Main models that are used almost everywhere \u2502 \u2514\u2500\u2500 schemas - Schemas for using in web routes \u2502 \u251c\u2500\u2500 resources - Strings that are used e.g., in web responses \u2502 \u251c\u2500\u2500 services - Logic that is not only CRUD related \u2502 \u251c\u2500\u2500 tests_ma - Unit tests \u2502 \u2514\u2500\u2500 main.py - FastAPI application creation and configuration","title":"API application folder structure"},{"location":"tre-developers/api/#auth-in-code","text":"The bulk of the authentication and authorization (A&A) related code of the API is located in /api_app/services/ folder. The A&A code has an abstract base for enabling the possibility to add additional A&A service providers. The Azure Active Directory (AAD) specific implementation is derived as follows: AccessService (access_service.py) <\u2500\u2500\u2500 AADAccessService (aad_access_service.py) fastapi.security.OAuth2AuthorizationCodeBearer <\u2500\u2500\u2500 AzureADAuthorization (aad_authentication.py) All the sensitive routes (API calls that can query sensitive data or modify resources) in the TRE API depend on having a \"current user\" authenticated. E.g., in /api_app/api/routes/workspaces.py : router = APIRouter ( dependencies = [ Depends ( get_current_user )]) Where APIRouter is part of the FastAPI . The user details, once authenticated, are stored as an instance of the custom User class.","title":"Auth in code"},{"location":"tre-developers/api/#auth-in-workspace-requests","text":"Some workspace routes require authConfig field in the request body. The AAD specific implementation expects a dictionary inside data field to contain the application (client) ID of the app registration associated with workspace : { \"authConfig\" : { \"provider\" : \"AAD\" , \"data\" : { \"app_id\" : \"c36f2ee3-8ec3-4431-9240-b1c0a0eb80a0\" } } } Caution The app registration for a workspace is not created by the API. One needs to be present (created manually) before using the API to provision a new workspace.","title":"Auth in workspace requests"},{"location":"tre-developers/api/#network-requirements","text":"To be able to run the TRE API it needs to access the following resource outside the Azure TRE VNET via explicit allowed Service Tags or URLs. Service Tag / Destination Justification AzureActiveDirectory Authenticate with the User Assigned identity to access Azure Cosmos DB and Azure Service Bus. AzureMonitor Publish traces and logs to one central place for troubleshooting. AzureResourceManager To perform control plane operations, such as create database in State Store. AzureContainerRegistry Pull the TRE API container image, as it is located in Azure Container Registry. graph.microsoft.com Lookup role assignments for Azure Active Directory user, to only show TRE resources and user has access to.","title":"Network requirements"},{"location":"tre-developers/dev-environment/","text":"Setting up your dev environment The supported development environments for Azure TRE are: Dev container Local development GitHub Codespaces Prerequisites Regardless of the development environment you choose, you will still need to fulfill the following prerequisites: An Azure subscription Azure Active Directory (AAD) with service principals created as explained in Authentication & authorization Obtain the source Copy the source or clone the repository to your local machine or choose to use the pre-configured dev container via GitHub Codespaces . Dev container The files for a dev container that comes with the required tools installed is located in /.devcontainer/ folder. Your native environment will require Docker and IDE supporting development in containers such as Visual Studio Code . See Developing inside a Container for instructions how to use the dev container with Visual Studio Code. (Optional) Install pre-commit hooks Pre commit hooks help you lint your python code on each git commit, to avoid having to fail the build when submitting a PR. Installing pre-commit hooks is completely optional. pre-commit install","title":"Dev Environment"},{"location":"tre-developers/dev-environment/#setting-up-your-dev-environment","text":"The supported development environments for Azure TRE are: Dev container Local development GitHub Codespaces","title":"Setting up your dev environment"},{"location":"tre-developers/dev-environment/#prerequisites","text":"Regardless of the development environment you choose, you will still need to fulfill the following prerequisites: An Azure subscription Azure Active Directory (AAD) with service principals created as explained in Authentication & authorization","title":"Prerequisites"},{"location":"tre-developers/dev-environment/#obtain-the-source","text":"Copy the source or clone the repository to your local machine or choose to use the pre-configured dev container via GitHub Codespaces .","title":"Obtain the source"},{"location":"tre-developers/dev-environment/#dev-container","text":"The files for a dev container that comes with the required tools installed is located in /.devcontainer/ folder. Your native environment will require Docker and IDE supporting development in containers such as Visual Studio Code . See Developing inside a Container for instructions how to use the dev container with Visual Studio Code.","title":"Dev container"},{"location":"tre-developers/dev-environment/#optional-install-pre-commit-hooks","text":"Pre commit hooks help you lint your python code on each git commit, to avoid having to fail the build when submitting a PR. Installing pre-commit hooks is completely optional. pre-commit install","title":"(Optional) Install pre-commit hooks"},{"location":"tre-developers/end-to-end-tests/","text":"End-to-end (E2E) tests Prerequisites Authentication and Authorization configuration set up as noted here An Azure Tre deployed environment. Running the End-to-End tests locally Navigate to the e2e_tests folder: cd e2e_tests Define the following environment variables: Environment variable name Description Example value RESOURCE_LOCATION The Azure Tre deployed environment LOCATION . eastus TRE_ID The Azure TRE instance name - used for deployment of resources (can be set to anything when debugging locally). mytre-dev-3142 RESOURCE The application (client) ID of the TRE API service principal. AUTH_TENANT_ID The tenant ID of the Azure AD. CLIENT_ID The application (client) ID of the E2E Test app service principal. SCOPE Scope(s) for the token. api://<TRE API app client ID>/Workspace.Read api://<TRE API app client ID>/Workspace.Write USERNAME The username of the E2E User . PASSWORD The password of the E2E User . AUTH_APP_CLIENT_ID The application (client) ID of the workspaces app . ACR_NAME The name of the TRE container registry. Run the E2E tests: PYTHONPATH = . python -m pytest --junit-xml pytest_e2e.xml","title":"End to End Tests"},{"location":"tre-developers/end-to-end-tests/#end-to-end-e2e-tests","text":"","title":"End-to-end (E2E) tests"},{"location":"tre-developers/end-to-end-tests/#prerequisites","text":"Authentication and Authorization configuration set up as noted here An Azure Tre deployed environment.","title":"Prerequisites"},{"location":"tre-developers/end-to-end-tests/#running-the-end-to-end-tests-locally","text":"Navigate to the e2e_tests folder: cd e2e_tests Define the following environment variables: Environment variable name Description Example value RESOURCE_LOCATION The Azure Tre deployed environment LOCATION . eastus TRE_ID The Azure TRE instance name - used for deployment of resources (can be set to anything when debugging locally). mytre-dev-3142 RESOURCE The application (client) ID of the TRE API service principal. AUTH_TENANT_ID The tenant ID of the Azure AD. CLIENT_ID The application (client) ID of the E2E Test app service principal. SCOPE Scope(s) for the token. api://<TRE API app client ID>/Workspace.Read api://<TRE API app client ID>/Workspace.Write USERNAME The username of the E2E User . PASSWORD The password of the E2E User . AUTH_APP_CLIENT_ID The application (client) ID of the workspaces app . ACR_NAME The name of the TRE container registry. Run the E2E tests: PYTHONPATH = . python -m pytest --junit-xml pytest_e2e.xml","title":"Running the End-to-End tests locally"},{"location":"tre-developers/resource-processor/","text":"Resource Processor (VMSS) Build and run the container Navigate to resource_processor/ folder and run docker build command: docker build -t resource-processor-vm-porter -f ./vmss_porter/Dockerfile . Run the image: docker run -it -v /var/run/docker.sock:/var/run/docker.sock --env-file .env resource-processor-vm-porter Local development To work locally checkout the source code and run: pip install -r ./vmss_porter/requirements.txt If you use visual studio code you can set up your launch.json to include the following block which will enable launching and debugging. { \"name\" : \"VMSS Processor\" , \"type\" : \"python\" , \"request\" : \"launch\" , \"program\" : \"vmss_porter/runner.py\" , \"console\" : \"integratedTerminal\" , \"cwd\" : \"${workspaceFolder}/resource_processor\" , \"env\" : { \"PYTHONPATH\" : \".\" , \"AZURE_CLIENT_ID\" : \"\" , \"AZURE_CLIENT_SECRET\" : \"\" , \"AZURE_TENANT_ID\" : \"\" , \"REGISTRY_SERVER\" : \"\" , \"TERRAFORM_STATE_CONTAINER_NAME\" : \"\" , \"MGMT_RESOURCE_GROUP_NAME\" : \"\" , \"MGMT_STORAGE_ACCOUNT_NAME\" : \"\" , \"SERVICE_BUS_DEPLOYMENT_STATUS_UPDATE_QUEUE\" : \"deploymentstatus\" , \"SERVICE_BUS_RESOURCE_REQUEST_QUEUE\" : \"workspacequeue\" , \"SERVICE_BUS_FULLY_QUALIFIED_NAMESPACE\" : \"\" , \"ARM_CLIENT_ID\" : \"\" , \"ARM_CLIENT_SECRET\" : \"\" , \"ARM_TENANT_ID\" : \"\" , \"ARM_SUBSCRIPTION_ID\" : \"\" , \"ARM_USE_MSI\" : \"false\" } } When working locally we use a service principal (SP). This SP needs enough permissions to be able to talk to service bus and to deploy resources into the subscription. That means the service principal needs Owner access to subscription(ARM_SUBSCRIPTION_ID) and also needs Azure Service Bus Data Sender and Azure Service Bus Data Receiver on the service bus namespace defined above (SERVICE_BUS_FULLY_QUALIFIED_NAMESPACE). Once the above is set up you can simulate receiving messages from service bus by going to service bus explorer on the portal and using a message payload for SERVICE_BUS_RESOURCE_REQUEST_QUEUE as follows { \"action\" : \"install\" , \"id\" : \"a8911125-50b4-491b-9e7c-ed8ff42220f9\" , \"name\" : \"tre-workspace-base\" , \"version\" : \"0.1.0\" , \"parameters\" : { \"azure_location\" : \"westeurope\" , \"workspace_id\" : \"20f9\" , \"tre_id\" : \"myfavtre\" , \"address_space\" : \"192.168.3.0/24\" }} This will trigger receiving of messages, and you can freely debug the code by setting breakpoints as desired. Porter Azure plugin Resource Processor uses Porter Azure plugin to store Porter data in TRE management storage account. The storage container, named porter , is created during the bootstrapping phase of TRE deployment. The /resource_processor/run.sh script generates a config.toml file in Porter home folder to enable the Azure plugin when the image is started. Debugging the deployed processor on Azure See the debugging and troubleshooting guide Network requirements To be able to run the Resource Processor it needs to access the following resource outside the Azure TRE VNET via explicit allowed Service Tags or URLs. Service Tag Justification AzureActiveDirectory Authenticate with the User Assigned identity to access Azure Resource Manager and Azure Service Bus. AzureResourceManager Access the Azure control plane to deploy and manage Azure resources. AzureMonitor Publish traces and logs to one central place for troubleshooting. AzureContainerRegistry Pull the Resource Processor container image, as it is located in Azure Container Registry. Storage The Porter bundles stores state between executions in an Azure Storage Account. AzureKeyVault The Porter bundles might need to create an Azure Key Vault inside of the Workspace. To verify the creation, before a private link connection is created, Terraform needs to reach Key Vault over public network To be able to install Docker, Porter and related packages ( script ) on the Resource Processor, the VM must have access to download from the following URLs: packages.microsoft.com keyserver.ubuntu.com api.snapcraft.io azure.archive.ubuntu.com security.ubuntu.com entropy.ubuntu.com download.docker.com registry-1.docker.io auth.docker.io registry.terraform.io releases.hashicorp.com","title":"Resource Processor"},{"location":"tre-developers/resource-processor/#resource-processor-vmss","text":"","title":"Resource Processor (VMSS)"},{"location":"tre-developers/resource-processor/#build-and-run-the-container","text":"Navigate to resource_processor/ folder and run docker build command: docker build -t resource-processor-vm-porter -f ./vmss_porter/Dockerfile . Run the image: docker run -it -v /var/run/docker.sock:/var/run/docker.sock --env-file .env resource-processor-vm-porter","title":"Build and run the container"},{"location":"tre-developers/resource-processor/#local-development","text":"To work locally checkout the source code and run: pip install -r ./vmss_porter/requirements.txt If you use visual studio code you can set up your launch.json to include the following block which will enable launching and debugging. { \"name\" : \"VMSS Processor\" , \"type\" : \"python\" , \"request\" : \"launch\" , \"program\" : \"vmss_porter/runner.py\" , \"console\" : \"integratedTerminal\" , \"cwd\" : \"${workspaceFolder}/resource_processor\" , \"env\" : { \"PYTHONPATH\" : \".\" , \"AZURE_CLIENT_ID\" : \"\" , \"AZURE_CLIENT_SECRET\" : \"\" , \"AZURE_TENANT_ID\" : \"\" , \"REGISTRY_SERVER\" : \"\" , \"TERRAFORM_STATE_CONTAINER_NAME\" : \"\" , \"MGMT_RESOURCE_GROUP_NAME\" : \"\" , \"MGMT_STORAGE_ACCOUNT_NAME\" : \"\" , \"SERVICE_BUS_DEPLOYMENT_STATUS_UPDATE_QUEUE\" : \"deploymentstatus\" , \"SERVICE_BUS_RESOURCE_REQUEST_QUEUE\" : \"workspacequeue\" , \"SERVICE_BUS_FULLY_QUALIFIED_NAMESPACE\" : \"\" , \"ARM_CLIENT_ID\" : \"\" , \"ARM_CLIENT_SECRET\" : \"\" , \"ARM_TENANT_ID\" : \"\" , \"ARM_SUBSCRIPTION_ID\" : \"\" , \"ARM_USE_MSI\" : \"false\" } } When working locally we use a service principal (SP). This SP needs enough permissions to be able to talk to service bus and to deploy resources into the subscription. That means the service principal needs Owner access to subscription(ARM_SUBSCRIPTION_ID) and also needs Azure Service Bus Data Sender and Azure Service Bus Data Receiver on the service bus namespace defined above (SERVICE_BUS_FULLY_QUALIFIED_NAMESPACE). Once the above is set up you can simulate receiving messages from service bus by going to service bus explorer on the portal and using a message payload for SERVICE_BUS_RESOURCE_REQUEST_QUEUE as follows { \"action\" : \"install\" , \"id\" : \"a8911125-50b4-491b-9e7c-ed8ff42220f9\" , \"name\" : \"tre-workspace-base\" , \"version\" : \"0.1.0\" , \"parameters\" : { \"azure_location\" : \"westeurope\" , \"workspace_id\" : \"20f9\" , \"tre_id\" : \"myfavtre\" , \"address_space\" : \"192.168.3.0/24\" }} This will trigger receiving of messages, and you can freely debug the code by setting breakpoints as desired.","title":"Local development"},{"location":"tre-developers/resource-processor/#porter-azure-plugin","text":"Resource Processor uses Porter Azure plugin to store Porter data in TRE management storage account. The storage container, named porter , is created during the bootstrapping phase of TRE deployment. The /resource_processor/run.sh script generates a config.toml file in Porter home folder to enable the Azure plugin when the image is started.","title":"Porter Azure plugin"},{"location":"tre-developers/resource-processor/#debugging-the-deployed-processor-on-azure","text":"See the debugging and troubleshooting guide","title":"Debugging the deployed processor on Azure"},{"location":"tre-developers/resource-processor/#network-requirements","text":"To be able to run the Resource Processor it needs to access the following resource outside the Azure TRE VNET via explicit allowed Service Tags or URLs. Service Tag Justification AzureActiveDirectory Authenticate with the User Assigned identity to access Azure Resource Manager and Azure Service Bus. AzureResourceManager Access the Azure control plane to deploy and manage Azure resources. AzureMonitor Publish traces and logs to one central place for troubleshooting. AzureContainerRegistry Pull the Resource Processor container image, as it is located in Azure Container Registry. Storage The Porter bundles stores state between executions in an Azure Storage Account. AzureKeyVault The Porter bundles might need to create an Azure Key Vault inside of the Workspace. To verify the creation, before a private link connection is created, Terraform needs to reach Key Vault over public network To be able to install Docker, Porter and related packages ( script ) on the Resource Processor, the VM must have access to download from the following URLs: packages.microsoft.com keyserver.ubuntu.com api.snapcraft.io azure.archive.ubuntu.com security.ubuntu.com entropy.ubuntu.com download.docker.com registry-1.docker.io auth.docker.io registry.terraform.io releases.hashicorp.com","title":"Network requirements"},{"location":"tre-workspace-authors/authoring-workspace-templates/","text":"Authoring workspaces templates Azure TRE workspaces, workspace services, and user resources are Porter bundles that in turn are based on Cloud Native Application Bundles (CNAB) . Workspace authors are free to choose the technology stack for provisioning resources (e.g., ARM templates, Terraform etc.), but the Azure TRE framework sets certain requirements for the bundle manifests, which specify the credentials, input and output parameters, deployment actions among other things. This document describes the requirements, and the process to author a template. Tip Use the base workspace bundle as reference or as the basis for the new bundle. To create a bundle from scratch follow the Porter Quickstart Guide ( porter create CLI command will generate a new bundle in the current directory). Prerequisites Docker installed Porter installed Azure TRE instance deployed to test against Workspace bundle manifest The manifest of a workspace bundle is the porter.yaml file (see Author Bundles in Porter documentation ). This section describes the mandatory credentials, input and output parameters of a TRE workspace bundle. Credentials A workspace bundle requires the following credentials to provision resources in Azure: Azure tenant ID Azure subscription ID The client ID of a service principal with privileges to provision resources The client secret (password) of a service principal The credentials are provided as environment variables by the deployment runner. The bundle author must use the following environment variable names: ARM_TENANT_ID ARM_SUBSCRIPTION_ID ARM_CLIENT_ID ARM_CLIENT_SECRET The names of the Porter credentials ( name field in porter.yaml ) can be freely chosen by the author. Example: credentials : - name : azure_tenant_id env : ARM_TENANT_ID - name : azure_subscription_id env : ARM_SUBSCRIPTION_ID - name : azure_client_id env : ARM_CLIENT_ID - name : azure_client_secret env : ARM_CLIENT_SECRET Parameters This section describes the mandatory (input) parameters of a workspace bundle manifest. Parameter Type Description Example value tre_id string Unique ID of for the TRE instance. tre-dev-42 workspace_id string Unique 4-character long, alphanumeric workspace ID. 0a9e azure_location string Azure location (region) to deploy the workspace resource to. westeurope address_space string VNet address space for the workspace services. 10.2.1.0/24 tre_id can be found in the resource names of the Azure TRE instance; for example the resource group name of the Azure TRE instance based on the example in the above table would be \" rg-tre-dev-42 \". Similarly to tre_id , workspace_id is used in the resource names of the workspace. The resource group name of the workspace must be of form \" rg-<tre_id>-ws-<workspace_id> \", for example: \" rg-tre-dev-42-ws-0a9e \". All the values for the required parameters will be provided by the deployment runner. Any custom parameters are picked up by Azure TRE API and will be queried from the user deploying the workspace bundle so make sure to write clear descriptions of the parameters as these are shown in the user interface to guide the user. Output Todo After a workspace with virtual machines is implemented this section can be written based on that. ( Outputs in Porter documentation to be linked here too.) Actions The required actions are the main two of CNAB spec: install - Deploys/repairs the workspace Azure resources, and must be idempotent uninstall - Tears down (deletes) the Azure resources of the workspace and its services Workspace service bundle manifests Workspace service bundles are generated in the same way as workspace bundles. The mandatory parameters for workspace services are: Parameter Type Description Example value tre_id string Unique ID of for the TRE instance. tre-dev-42 workspace_id string Unique 4-character long, alphanumeric workspace ID. 0a9e User resource bundle manifests User Resource bundles are generated in the same way as workspace bundles and workspace services bundles. The main difference is that a workspace service type needs to be supplied when registering a user resource template, as it only applies to a given workspace service. The mandatory parameters for User Resources are: Parameter Type Description Example value tre_id string Unique ID of for the TRE instance. tre-dev-42 workspace_id string Unique 4-character long, alphanumeric workspace ID. 0a9e Supported Porter mixins The deployment runner of Azure TRE supports the following Porter mixins : exec az arm terraform To add support for additional mixins including custom ones, the TRE Porter installation script /devops/scripts/install_porter.sh needs to be modified. Versioning Workspace versions are the bundle versions specified in the metadata . The bundle versions should match the image tags in the container registry (see Publishing workspace bundle ). TRE does not provide means to update an existing workspace to a newer version. Instead, the user has to first uninstall the old version and then install the new one. The CNAB upgrade or a Porter custom (\" update \") action may be used in the future version of TRE to do this automatically. Publishing workspace bundle See Registering workspace templates .","title":"Authoring Workspace Templates"},{"location":"tre-workspace-authors/authoring-workspace-templates/#authoring-workspaces-templates","text":"Azure TRE workspaces, workspace services, and user resources are Porter bundles that in turn are based on Cloud Native Application Bundles (CNAB) . Workspace authors are free to choose the technology stack for provisioning resources (e.g., ARM templates, Terraform etc.), but the Azure TRE framework sets certain requirements for the bundle manifests, which specify the credentials, input and output parameters, deployment actions among other things. This document describes the requirements, and the process to author a template. Tip Use the base workspace bundle as reference or as the basis for the new bundle. To create a bundle from scratch follow the Porter Quickstart Guide ( porter create CLI command will generate a new bundle in the current directory).","title":"Authoring workspaces templates"},{"location":"tre-workspace-authors/authoring-workspace-templates/#prerequisites","text":"Docker installed Porter installed Azure TRE instance deployed to test against","title":"Prerequisites"},{"location":"tre-workspace-authors/authoring-workspace-templates/#workspace-bundle-manifest","text":"The manifest of a workspace bundle is the porter.yaml file (see Author Bundles in Porter documentation ). This section describes the mandatory credentials, input and output parameters of a TRE workspace bundle.","title":"Workspace bundle manifest"},{"location":"tre-workspace-authors/authoring-workspace-templates/#credentials","text":"A workspace bundle requires the following credentials to provision resources in Azure: Azure tenant ID Azure subscription ID The client ID of a service principal with privileges to provision resources The client secret (password) of a service principal The credentials are provided as environment variables by the deployment runner. The bundle author must use the following environment variable names: ARM_TENANT_ID ARM_SUBSCRIPTION_ID ARM_CLIENT_ID ARM_CLIENT_SECRET The names of the Porter credentials ( name field in porter.yaml ) can be freely chosen by the author. Example: credentials : - name : azure_tenant_id env : ARM_TENANT_ID - name : azure_subscription_id env : ARM_SUBSCRIPTION_ID - name : azure_client_id env : ARM_CLIENT_ID - name : azure_client_secret env : ARM_CLIENT_SECRET","title":"Credentials"},{"location":"tre-workspace-authors/authoring-workspace-templates/#parameters","text":"This section describes the mandatory (input) parameters of a workspace bundle manifest. Parameter Type Description Example value tre_id string Unique ID of for the TRE instance. tre-dev-42 workspace_id string Unique 4-character long, alphanumeric workspace ID. 0a9e azure_location string Azure location (region) to deploy the workspace resource to. westeurope address_space string VNet address space for the workspace services. 10.2.1.0/24 tre_id can be found in the resource names of the Azure TRE instance; for example the resource group name of the Azure TRE instance based on the example in the above table would be \" rg-tre-dev-42 \". Similarly to tre_id , workspace_id is used in the resource names of the workspace. The resource group name of the workspace must be of form \" rg-<tre_id>-ws-<workspace_id> \", for example: \" rg-tre-dev-42-ws-0a9e \". All the values for the required parameters will be provided by the deployment runner. Any custom parameters are picked up by Azure TRE API and will be queried from the user deploying the workspace bundle so make sure to write clear descriptions of the parameters as these are shown in the user interface to guide the user.","title":"Parameters"},{"location":"tre-workspace-authors/authoring-workspace-templates/#output","text":"Todo After a workspace with virtual machines is implemented this section can be written based on that. ( Outputs in Porter documentation to be linked here too.)","title":"Output"},{"location":"tre-workspace-authors/authoring-workspace-templates/#actions","text":"The required actions are the main two of CNAB spec: install - Deploys/repairs the workspace Azure resources, and must be idempotent uninstall - Tears down (deletes) the Azure resources of the workspace and its services","title":"Actions"},{"location":"tre-workspace-authors/authoring-workspace-templates/#workspace-service-bundle-manifests","text":"Workspace service bundles are generated in the same way as workspace bundles. The mandatory parameters for workspace services are: Parameter Type Description Example value tre_id string Unique ID of for the TRE instance. tre-dev-42 workspace_id string Unique 4-character long, alphanumeric workspace ID. 0a9e","title":"Workspace service bundle manifests"},{"location":"tre-workspace-authors/authoring-workspace-templates/#user-resource-bundle-manifests","text":"User Resource bundles are generated in the same way as workspace bundles and workspace services bundles. The main difference is that a workspace service type needs to be supplied when registering a user resource template, as it only applies to a given workspace service. The mandatory parameters for User Resources are: Parameter Type Description Example value tre_id string Unique ID of for the TRE instance. tre-dev-42 workspace_id string Unique 4-character long, alphanumeric workspace ID. 0a9e","title":"User resource bundle manifests"},{"location":"tre-workspace-authors/authoring-workspace-templates/#supported-porter-mixins","text":"The deployment runner of Azure TRE supports the following Porter mixins : exec az arm terraform To add support for additional mixins including custom ones, the TRE Porter installation script /devops/scripts/install_porter.sh needs to be modified.","title":"Supported Porter mixins"},{"location":"tre-workspace-authors/authoring-workspace-templates/#versioning","text":"Workspace versions are the bundle versions specified in the metadata . The bundle versions should match the image tags in the container registry (see Publishing workspace bundle ). TRE does not provide means to update an existing workspace to a newer version. Instead, the user has to first uninstall the old version and then install the new one. The CNAB upgrade or a Porter custom (\" update \") action may be used in the future version of TRE to do this automatically.","title":"Versioning"},{"location":"tre-workspace-authors/authoring-workspace-templates/#publishing-workspace-bundle","text":"See Registering workspace templates .","title":"Publishing workspace bundle"},{"location":"tre-workspace-authors/firewall-rules/","text":"Adding Firewall Rules as part of a workspace or service deployment A TRE service may require certain firewall rules to be opened in the TRE firewall. Examples include: Access to an external authorisation endpoint Access to an external data store Access to an external API Please be aware when opening firewall rules there is the potential for data to be leaked from the workspace to the external location. Using Terraform to open firewall rules Until a mechanism to update shared services has been implemented, firewall rule updates should be done using terraform as part of the service deployment. The aim is to create a firewall rule that grants access from the workspace's address space to the external location. A challenge with this is that the rule must use a priority that has not been used by any other rule. Create a firewall.tf file in the terraform directory of the workspace. Add the following code to the firewall.tf file to enable the TRE firewall and workspace network to be referenced: data \"azurerm_firewall\" \"fw\" { name = \"fw-${var.tre_id}\" resource_group_name = \"rg-${var.tre_id}\" } data \"azurerm_virtual_network\" \"ws\" { name = \"vnet-${var.tre_id}-ws-${var.workspace_id}\" resource_group_name = data.azurerm_resource_group.ws.name } Define a local variable that contains the locations that access should be allowed to, and the naming format for the service resources for example: locals { allowed_urls = [ \"*.anaconda.com\", \"*.anaconda.org\" ] service_resource_name_suffix = \"${var.tre_id}-ws-${var.workspace_id}-svc-${local.service_id}\" } Log into the Azure CLI using service principal details: resource \"null_resource\" \"az_login\" { provisioner \"local-exec\" { command = \"az login --identity -u '${var.arm_client_id}'\" } triggers = { timestamp = timestamp () } } Call the get_firewall_priorities.sh script to find the next available priority: data \"external\" \"rule_priorities\" { program = [ \"bash\", \"-c\", \"./get_firewall_priorities.sh\" ] query = { firewall_name = data.azurerm_firewall.fw.name resource_group_name = data.azurerm_firewall.fw.resource_group_name service_resource_name_suffix = local.service_resource_name_suffix } depends_on = [ null_resource.az_login ] } Save the get_firewall_priorities.sh script as a file in the terraform directory: #!/bin/bash set -e eval \" $( jq -r '@sh \"firewall_name=\\(.firewall_name) resource_group_name=\\(.resource_group_name) service_resource_name_suffix=\\(.service_resource_name_suffix)\"' ) \" if NETWORK_RULES = $( az network firewall network-rule list -g $resource_group_name -f $firewall_name --collection-name \"nrc- $service_resource_name_suffix \" -o json ) ; then NETWORK_RULE_PRIORITY = $( echo $NETWORK_RULES | jq '.priority' ) else NETWORK_RULE_MAX_PRIORITY = $( az network firewall network-rule collection list -f $firewall_name -g $resource_group_name -o json --query 'not_null(max_by([],&priority).priority) || `100`' ) NETWORK_RULE_PRIORITY = $(( $NETWORK_RULE_MAX_PRIORITY + 1 )) fi if APPLICATION_RULES = $( az network firewall application-rule list -g $resource_group_name -f $firewall_name --collection-name \"arc- $service_resource_name_suffix \" -o json ) ; then APPLICATION_RULE_PRIORITY = $( echo $APPLICATION_RULES | jq '.priority' ) else APPLICATION_RULE_MAX_PRIORITY = $( az network firewall application-rule collection list -f $firewall_name -g $resource_group_name -o json --query 'not_null(max_by([],&priority).priority) || `100`' ) APPLICATION_RULE_PRIORITY = $(( $APPLICATION_RULE_MAX_PRIORITY + 1 )) fi # Safely produce a JSON object containing the result value. jq -n --arg network_rule_priority \" $NETWORK_RULE_PRIORITY \" --arg application_rule_priority \" $APPLICATION_RULE_PRIORITY \" '{ \"network_rule_priority\":$network_rule_priority, \"application_rule_priority\":$application_rule_priority }' Create the firewall rule using a resource similar to the below: resource \"azurerm_firewall_application_rule_collection\" \"apprulecollection\" { name = \"arc-${local.service_resource_name_suffix}\" azure_firewall_name = data.azurerm_firewall.fw.name resource_group_name = data.azurerm_firewall.fw.resource_group_name priority = data.external.rule_priorities.result.application_rule_priority action = \"Allow\" rule { name = \"allowServiceXRules\" source_addresses = data.azurerm_virtual_network.ws.address_space target_fqdns = local.allowed_urls protocol { port = \"443\" type = \"Https\" } protocol { port = \"80\" type = \"Http\" } } }","title":"Firewall Rules"},{"location":"tre-workspace-authors/firewall-rules/#adding-firewall-rules-as-part-of-a-workspace-or-service-deployment","text":"A TRE service may require certain firewall rules to be opened in the TRE firewall. Examples include: Access to an external authorisation endpoint Access to an external data store Access to an external API Please be aware when opening firewall rules there is the potential for data to be leaked from the workspace to the external location.","title":"Adding Firewall Rules as part of a workspace or service deployment"},{"location":"tre-workspace-authors/firewall-rules/#using-terraform-to-open-firewall-rules","text":"Until a mechanism to update shared services has been implemented, firewall rule updates should be done using terraform as part of the service deployment. The aim is to create a firewall rule that grants access from the workspace's address space to the external location. A challenge with this is that the rule must use a priority that has not been used by any other rule. Create a firewall.tf file in the terraform directory of the workspace. Add the following code to the firewall.tf file to enable the TRE firewall and workspace network to be referenced: data \"azurerm_firewall\" \"fw\" { name = \"fw-${var.tre_id}\" resource_group_name = \"rg-${var.tre_id}\" } data \"azurerm_virtual_network\" \"ws\" { name = \"vnet-${var.tre_id}-ws-${var.workspace_id}\" resource_group_name = data.azurerm_resource_group.ws.name } Define a local variable that contains the locations that access should be allowed to, and the naming format for the service resources for example: locals { allowed_urls = [ \"*.anaconda.com\", \"*.anaconda.org\" ] service_resource_name_suffix = \"${var.tre_id}-ws-${var.workspace_id}-svc-${local.service_id}\" } Log into the Azure CLI using service principal details: resource \"null_resource\" \"az_login\" { provisioner \"local-exec\" { command = \"az login --identity -u '${var.arm_client_id}'\" } triggers = { timestamp = timestamp () } } Call the get_firewall_priorities.sh script to find the next available priority: data \"external\" \"rule_priorities\" { program = [ \"bash\", \"-c\", \"./get_firewall_priorities.sh\" ] query = { firewall_name = data.azurerm_firewall.fw.name resource_group_name = data.azurerm_firewall.fw.resource_group_name service_resource_name_suffix = local.service_resource_name_suffix } depends_on = [ null_resource.az_login ] } Save the get_firewall_priorities.sh script as a file in the terraform directory: #!/bin/bash set -e eval \" $( jq -r '@sh \"firewall_name=\\(.firewall_name) resource_group_name=\\(.resource_group_name) service_resource_name_suffix=\\(.service_resource_name_suffix)\"' ) \" if NETWORK_RULES = $( az network firewall network-rule list -g $resource_group_name -f $firewall_name --collection-name \"nrc- $service_resource_name_suffix \" -o json ) ; then NETWORK_RULE_PRIORITY = $( echo $NETWORK_RULES | jq '.priority' ) else NETWORK_RULE_MAX_PRIORITY = $( az network firewall network-rule collection list -f $firewall_name -g $resource_group_name -o json --query 'not_null(max_by([],&priority).priority) || `100`' ) NETWORK_RULE_PRIORITY = $(( $NETWORK_RULE_MAX_PRIORITY + 1 )) fi if APPLICATION_RULES = $( az network firewall application-rule list -g $resource_group_name -f $firewall_name --collection-name \"arc- $service_resource_name_suffix \" -o json ) ; then APPLICATION_RULE_PRIORITY = $( echo $APPLICATION_RULES | jq '.priority' ) else APPLICATION_RULE_MAX_PRIORITY = $( az network firewall application-rule collection list -f $firewall_name -g $resource_group_name -o json --query 'not_null(max_by([],&priority).priority) || `100`' ) APPLICATION_RULE_PRIORITY = $(( $APPLICATION_RULE_MAX_PRIORITY + 1 )) fi # Safely produce a JSON object containing the result value. jq -n --arg network_rule_priority \" $NETWORK_RULE_PRIORITY \" --arg application_rule_priority \" $APPLICATION_RULE_PRIORITY \" '{ \"network_rule_priority\":$network_rule_priority, \"application_rule_priority\":$application_rule_priority }' Create the firewall rule using a resource similar to the below: resource \"azurerm_firewall_application_rule_collection\" \"apprulecollection\" { name = \"arc-${local.service_resource_name_suffix}\" azure_firewall_name = data.azurerm_firewall.fw.name resource_group_name = data.azurerm_firewall.fw.resource_group_name priority = data.external.rule_priorities.result.application_rule_priority action = \"Allow\" rule { name = \"allowServiceXRules\" source_addresses = data.azurerm_virtual_network.ws.address_space target_fqdns = local.allowed_urls protocol { port = \"443\" type = \"Https\" } protocol { port = \"80\" type = \"Http\" } } }","title":"Using Terraform to open firewall rules"},{"location":"tre-workspace-authors/registering-workspace-templates/","text":"Registering Workspace Templates To deploy a new type of Workspace, we need to register a Workspace Template using the API. Porter Bundles Porter bundles can either be registered interactively using the Swagger UI or automatically using the utility script (useful in CI/CD scenarios). The script is provided at /devops/scripts/publish_register_bundle.sh . The script can also be used to generate the payload required by the API without actually calling the API. The script carries out the following actions: Publishes the bundle to the Azure Container Registry specified. Extracts the parameters from the bundle using porter explain . Registration using Swagger UI We will use the utility script to generate the payload. The script needs to be executed from within the bundle directory, for example /templates/workspaces/azureml_devtestlabs/ . This script can be used as follows: ../../../devops/scripts/publish_register_bundle.sh -r <acr_name> -i -t workspace Copy the resulting payload json. Navigate to the Swagger UI at /docs Log into the Swagger UI by clicking Authorize , then Authorize again. You will be redirected to the login page. Once logged in. Click Try it out on the POST /api/workspace-templates operation: Paste the payload json generated earlier into the Request body field, then click Execute . Review the server response. To verify registration of the template do GET operation on /api/workspace-templates . The name of the template should now be listed. Registration using script To use the script to automatically register the template, a user that does not require an interactive login must be created as per the e2e test user documentation here . The script needs to be executed from within the bundle directory, for example /templates/workspaces/azureml_devtestlabs/ . This script can be used as follows: Usage: ../../../devops/scripts/publish_register_bundle.sh [-u --tre_url] [-c --current] [-i --insecure] Options: -r, --acr-name Azure Container Registry Name -t, --bundle-type Bundle type, workspace -c, --current: Make this the currently deployed version of this template -i, --insecure: Bypass SSL certificate checks -u, --tre_url: URL for the TRE (required for automatic registration) -a, --access-token Azure access token to automatically post to the API (required for automatic registration) In addition to generating the payload, the script posts the payload to the /api/workspace-templates endpoint. Once registered the template can be retrieved by a GET operation on /api/workspace-templates . Tip Follow the same procedure to register workspace service templates and user resource templates","title":"Registering Workspace Templates"},{"location":"tre-workspace-authors/registering-workspace-templates/#registering-workspace-templates","text":"To deploy a new type of Workspace, we need to register a Workspace Template using the API.","title":"Registering Workspace Templates"},{"location":"tre-workspace-authors/registering-workspace-templates/#porter-bundles","text":"Porter bundles can either be registered interactively using the Swagger UI or automatically using the utility script (useful in CI/CD scenarios). The script is provided at /devops/scripts/publish_register_bundle.sh . The script can also be used to generate the payload required by the API without actually calling the API. The script carries out the following actions: Publishes the bundle to the Azure Container Registry specified. Extracts the parameters from the bundle using porter explain .","title":"Porter Bundles"},{"location":"tre-workspace-authors/registering-workspace-templates/#registration-using-swagger-ui","text":"We will use the utility script to generate the payload. The script needs to be executed from within the bundle directory, for example /templates/workspaces/azureml_devtestlabs/ . This script can be used as follows: ../../../devops/scripts/publish_register_bundle.sh -r <acr_name> -i -t workspace Copy the resulting payload json. Navigate to the Swagger UI at /docs Log into the Swagger UI by clicking Authorize , then Authorize again. You will be redirected to the login page. Once logged in. Click Try it out on the POST /api/workspace-templates operation: Paste the payload json generated earlier into the Request body field, then click Execute . Review the server response. To verify registration of the template do GET operation on /api/workspace-templates . The name of the template should now be listed.","title":"Registration using Swagger UI"},{"location":"tre-workspace-authors/registering-workspace-templates/#registration-using-script","text":"To use the script to automatically register the template, a user that does not require an interactive login must be created as per the e2e test user documentation here . The script needs to be executed from within the bundle directory, for example /templates/workspaces/azureml_devtestlabs/ . This script can be used as follows: Usage: ../../../devops/scripts/publish_register_bundle.sh [-u --tre_url] [-c --current] [-i --insecure] Options: -r, --acr-name Azure Container Registry Name -t, --bundle-type Bundle type, workspace -c, --current: Make this the currently deployed version of this template -i, --insecure: Bypass SSL certificate checks -u, --tre_url: URL for the TRE (required for automatic registration) -a, --access-token Azure access token to automatically post to the API (required for automatic registration) In addition to generating the payload, the script posts the payload to the /api/workspace-templates endpoint. Once registered the template can be retrieved by a GET operation on /api/workspace-templates . Tip Follow the same procedure to register workspace service templates and user resource templates","title":"Registration using script"},{"location":"workspace-templates/user-resources/guacamole-win10-vm/","text":"Guacamole User Resource Service bundle (Windows 10) This is a User Resource Service template. It contains a Windows 10 to be used by TRE researchers and to be connected using a Guacamole server . It blocks all inbound and outbound traffic to the internet and allows only RDP connections from within the vnet. Firewall Rules Please be aware that the following Firewall rules are opened for the workspace when this service is deployed: Inbound connectivity from within the VNET to the RDP port Prerequisites A base workspace bundle installed A guacamole workspace service bundle installed Manual Deployment Create a copy of templates/workspace_services/guacamole/.env.sample with the name .env and update the variables with the appropriate values. Environment variable name Description WORKSPACE_ID The 4 character unique identifier used when deploying the base workspace bundle. PARENT_SERVICE_ID The unique identifier of this service parent (a Guacamole service) Build and install the Guacamole Service bundle make porter-build DIR=./templates/workspace_services/guacamole/user_resources/guacamole-azure-win10vm make porter-install DIR=./templates/workspace_services/guacamole/user_resources/guacamole-azure-win10vm","title":"Guacamole Win10 VM"},{"location":"workspace-templates/user-resources/guacamole-win10-vm/#guacamole-user-resource-service-bundle-windows-10","text":"This is a User Resource Service template. It contains a Windows 10 to be used by TRE researchers and to be connected using a Guacamole server . It blocks all inbound and outbound traffic to the internet and allows only RDP connections from within the vnet.","title":"Guacamole User Resource Service bundle (Windows 10)"},{"location":"workspace-templates/user-resources/guacamole-win10-vm/#firewall-rules","text":"Please be aware that the following Firewall rules are opened for the workspace when this service is deployed: Inbound connectivity from within the VNET to the RDP port","title":"Firewall Rules"},{"location":"workspace-templates/user-resources/guacamole-win10-vm/#prerequisites","text":"A base workspace bundle installed A guacamole workspace service bundle installed","title":"Prerequisites"},{"location":"workspace-templates/user-resources/guacamole-win10-vm/#manual-deployment","text":"Create a copy of templates/workspace_services/guacamole/.env.sample with the name .env and update the variables with the appropriate values. Environment variable name Description WORKSPACE_ID The 4 character unique identifier used when deploying the base workspace bundle. PARENT_SERVICE_ID The unique identifier of this service parent (a Guacamole service) Build and install the Guacamole Service bundle make porter-build DIR=./templates/workspace_services/guacamole/user_resources/guacamole-azure-win10vm make porter-install DIR=./templates/workspace_services/guacamole/user_resources/guacamole-azure-win10vm","title":"Manual Deployment"},{"location":"workspace-templates/workspace-services/azure-ml/","text":"Azure Machine Learning Service bundle See: https://azure.microsoft.com/services/machine-learning/ This service installs the following resources into an existing virtual network within the workspace: Firewall Rules Please be aware that the following Firewall rules are opened for the workspace when this service is deployed: URLs: graph.windows.net ml.azure.com login.microsoftonline.com aadcdn.msftauth.net graph.microsoft.com management.azure.com viennaglobal.azurecr.io Service Tags: Storage. {AzureRegion} AzureContainerRegistry Prerequisites A base workspace bundle installed Manual Deployment Create a copy of templates/workspace_services/azureml/.env.sample with the name .env and update the variables with the appropriate values. Environment variable name Description WORKSPACE_ID The 4 character unique identifier used when deploying the base workspace bundle. Build and install the Azure ML Service bundle make porter-build DIR=./templates/workspace_services/azureml make porter-install DIR=./templates/workspace_services/azureml","title":"Azure ML"},{"location":"workspace-templates/workspace-services/azure-ml/#azure-machine-learning-service-bundle","text":"See: https://azure.microsoft.com/services/machine-learning/ This service installs the following resources into an existing virtual network within the workspace:","title":"Azure Machine Learning Service bundle"},{"location":"workspace-templates/workspace-services/azure-ml/#firewall-rules","text":"Please be aware that the following Firewall rules are opened for the workspace when this service is deployed: URLs: graph.windows.net ml.azure.com login.microsoftonline.com aadcdn.msftauth.net graph.microsoft.com management.azure.com viennaglobal.azurecr.io Service Tags: Storage. {AzureRegion} AzureContainerRegistry","title":"Firewall Rules"},{"location":"workspace-templates/workspace-services/azure-ml/#prerequisites","text":"A base workspace bundle installed","title":"Prerequisites"},{"location":"workspace-templates/workspace-services/azure-ml/#manual-deployment","text":"Create a copy of templates/workspace_services/azureml/.env.sample with the name .env and update the variables with the appropriate values. Environment variable name Description WORKSPACE_ID The 4 character unique identifier used when deploying the base workspace bundle. Build and install the Azure ML Service bundle make porter-build DIR=./templates/workspace_services/azureml make porter-install DIR=./templates/workspace_services/azureml","title":"Manual Deployment"},{"location":"workspace-templates/workspace-services/dev-test-labs/","text":"Azure DevTest Labs Service bundle See: https://azure.microsoft.com/services/devtest-lab/ Prerequisites A base workspace bundle installed Manual Deployment Create a copy of templates/workspace_services/devtestlabs/.env.sample with the name .env and update with the Workspace ID used when deploying the base workspace. Environment variable name Description WORKSPACE_ID The 4 character unique identifier used when deploying the base workspace bundle. Build and install the Azure DevTest Labs Service bundle make porter-build DIR=./templates/workspace_services/devtestlabs make porter-install DIR=./templates/workspace_services/devtestlabs Create and expose a VM via the Firewall When this service used without a virtual desktop gateway it might be necessary to manually create and expose a VM via the TRE firewall. This method of exposing VMs is not recommended for large scale deployments given there will be multiple resources and rules to manually manage. Create a DevTest Labs VM and open a port in the TRE firewall using the script provided. Usage: ./create_and_expose_vm.sh [-l --lab-name] [-t --tre_id] [-w --workspace_id] [-n --vm-name] [-i --image-name] Options: -l, --lab-name: Name of the DevTest Lab -t, --tre_id ID of the TRE -w, --workspace_id ID of the workspace -n, --vm-name Name of the VM -i, --image-name: Name of the VM Image Example: ./templates/workspace_services/devtestlabs/create_and_expose_vm.sh --lab-name <lab_name> --tre-id <tre-id> --workspace-id <workspace-id> --vm-name <vmn-name> --image-name \"Data Science Virtual Machine - Windows Server 2019\" Using the details provided by the script, and a remote desktop connection client connect to the VM.","title":"DevTest Labs"},{"location":"workspace-templates/workspace-services/dev-test-labs/#azure-devtest-labs-service-bundle","text":"See: https://azure.microsoft.com/services/devtest-lab/","title":"Azure DevTest Labs Service bundle"},{"location":"workspace-templates/workspace-services/dev-test-labs/#prerequisites","text":"A base workspace bundle installed","title":"Prerequisites"},{"location":"workspace-templates/workspace-services/dev-test-labs/#manual-deployment","text":"Create a copy of templates/workspace_services/devtestlabs/.env.sample with the name .env and update with the Workspace ID used when deploying the base workspace. Environment variable name Description WORKSPACE_ID The 4 character unique identifier used when deploying the base workspace bundle. Build and install the Azure DevTest Labs Service bundle make porter-build DIR=./templates/workspace_services/devtestlabs make porter-install DIR=./templates/workspace_services/devtestlabs","title":"Manual Deployment"},{"location":"workspace-templates/workspace-services/dev-test-labs/#create-and-expose-a-vm-via-the-firewall","text":"When this service used without a virtual desktop gateway it might be necessary to manually create and expose a VM via the TRE firewall. This method of exposing VMs is not recommended for large scale deployments given there will be multiple resources and rules to manually manage. Create a DevTest Labs VM and open a port in the TRE firewall using the script provided. Usage: ./create_and_expose_vm.sh [-l --lab-name] [-t --tre_id] [-w --workspace_id] [-n --vm-name] [-i --image-name] Options: -l, --lab-name: Name of the DevTest Lab -t, --tre_id ID of the TRE -w, --workspace_id ID of the workspace -n, --vm-name Name of the VM -i, --image-name: Name of the VM Image Example: ./templates/workspace_services/devtestlabs/create_and_expose_vm.sh --lab-name <lab_name> --tre-id <tre-id> --workspace-id <workspace-id> --vm-name <vmn-name> --image-name \"Data Science Virtual Machine - Windows Server 2019\" Using the details provided by the script, and a remote desktop connection client connect to the VM.","title":"Create and expose a VM via the Firewall"},{"location":"workspace-templates/workspace-services/guacamole/","text":"Guacamole Service bundle See: https://guacamole.apache.org/ Firewall Rules Please be aware that the following Firewall rules are opened for the workspace when this service is deployed: URLs: Todo Add firewall rules Prerequisites A base workspace bundle installed Manual Deployment Create a copy of templates/workspace_services/guacamole/.env.sample with the name .env and update the variables with the appropriate values. Environment variable name Description WORKSPACE_ID The 4 character unique identifier used when deploying the base workspace bundle. GUACAMOLE_IMAGE_TAG Image tag of the Guacamole server Build and install the Guacamole Service bundle make porter-build DIR=./templates/workspace_services/guacamole make porter-install DIR=./templates/workspace_services/guacamole","title":"Guacamole"},{"location":"workspace-templates/workspace-services/guacamole/#guacamole-service-bundle","text":"See: https://guacamole.apache.org/","title":"Guacamole Service bundle"},{"location":"workspace-templates/workspace-services/guacamole/#firewall-rules","text":"Please be aware that the following Firewall rules are opened for the workspace when this service is deployed: URLs: Todo Add firewall rules","title":"Firewall Rules"},{"location":"workspace-templates/workspace-services/guacamole/#prerequisites","text":"A base workspace bundle installed","title":"Prerequisites"},{"location":"workspace-templates/workspace-services/guacamole/#manual-deployment","text":"Create a copy of templates/workspace_services/guacamole/.env.sample with the name .env and update the variables with the appropriate values. Environment variable name Description WORKSPACE_ID The 4 character unique identifier used when deploying the base workspace bundle. GUACAMOLE_IMAGE_TAG Image tag of the Guacamole server Build and install the Guacamole Service bundle make porter-build DIR=./templates/workspace_services/guacamole make porter-install DIR=./templates/workspace_services/guacamole","title":"Manual Deployment"},{"location":"workspace-templates/workspace-services/inner-eye-deep-learning/","text":"InnerEye DeepLearning Service Bundle See: https://github.com/microsoft/InnerEye-DeepLearning Firewall Rules Please be aware that the following Firewall rules are opened for the workspace when this service is deployed. These are all dependencies needed by InnerEye to be able to develop and train models: URLs: *.anaconda.com *.anaconda.org binstar-cio-packages-prod.s3.amazonaws.com github.com *pypi.org *pythonhosted.org github-cloud.githubusercontent.com Prerequisites A workspace with an Azure ML Service bundle installed Manual Deployment Create a copy of templates/workspace_services/innereye_deeplearning/.env.sample with the name .env and update the variables with the appropriate values. Environment variable name Description WORKSPACE_ID The 4 character unique identifier used when deploying the base workspace bundle. AZUREML_WORKSPACE_NAME Name of the Azure ML workspace deployed as part of the Azure ML workspace service prerequisite. AZUREML_ACR_ID Azure resource ID of the Azure Container Registry deployed as part of the Azure ML workspace service prerequisite. Build and install the InnerEye Deep Learning Service bundle make porter-build DIR=./templates/workspace_services/innereye_deeplearning make porter-publish DIR=./templates/workspace_services/innereye_deeplearning make porter-install DIR=./templates/workspace_services/innereye_deeplearning Running the InnerEye HelloWorld on AML Compute Cluster Log onto a VM in the workspace, open PowerShell and run: git clone https://github.com/microsoft/InnerEye-DeepLearning cd InnerEye-DeepLearning git lfs install git lfs pull conda init conda env create --file environment.yml Restart PowerShell and navigate to the \"InnerEye-DeepLearning\" folder conda activate InnerEye Open Azure Storage Explorer and connect to your Storage Account using name and access key On the storage account create a container with name datasets and a folder named hello_world Copy dataset.csv file from Tests/ML/test_data/dataset.csv to the hello_world folder Copy the whole train_and_test_data folder from Test/ML/test_data/train_and_test_data to the hello_world folder Update the following variables in InnerEye/settings.yml : subscription_id, resource_group, workspace_name, cluster (see AML setup for more details). Open your browser to ml.azure.com, login, select the right Subscription and AML workspace and then navigate to Data stores . Create a New datastore named innereyedatasets and link it to your storage account and datasets container. Back from PowerShell run python InnerEye/ML/runner.py --model=HelloWorld --azureml=True The runner will provide you with a link and ask you to open it to login. Copy the link and open it in browser (Edge) on the DSVM and login. The run will continue after login. In your browser navigate to ml.azure.com and open the Experiments tab to follow the progress of the training","title":"InnerEye Deep Learning"},{"location":"workspace-templates/workspace-services/inner-eye-deep-learning/#innereye-deeplearning-service-bundle","text":"See: https://github.com/microsoft/InnerEye-DeepLearning","title":"InnerEye DeepLearning Service Bundle"},{"location":"workspace-templates/workspace-services/inner-eye-deep-learning/#firewall-rules","text":"Please be aware that the following Firewall rules are opened for the workspace when this service is deployed. These are all dependencies needed by InnerEye to be able to develop and train models: URLs: *.anaconda.com *.anaconda.org binstar-cio-packages-prod.s3.amazonaws.com github.com *pypi.org *pythonhosted.org github-cloud.githubusercontent.com","title":"Firewall Rules"},{"location":"workspace-templates/workspace-services/inner-eye-deep-learning/#prerequisites","text":"A workspace with an Azure ML Service bundle installed","title":"Prerequisites"},{"location":"workspace-templates/workspace-services/inner-eye-deep-learning/#manual-deployment","text":"Create a copy of templates/workspace_services/innereye_deeplearning/.env.sample with the name .env and update the variables with the appropriate values. Environment variable name Description WORKSPACE_ID The 4 character unique identifier used when deploying the base workspace bundle. AZUREML_WORKSPACE_NAME Name of the Azure ML workspace deployed as part of the Azure ML workspace service prerequisite. AZUREML_ACR_ID Azure resource ID of the Azure Container Registry deployed as part of the Azure ML workspace service prerequisite. Build and install the InnerEye Deep Learning Service bundle make porter-build DIR=./templates/workspace_services/innereye_deeplearning make porter-publish DIR=./templates/workspace_services/innereye_deeplearning make porter-install DIR=./templates/workspace_services/innereye_deeplearning","title":"Manual Deployment"},{"location":"workspace-templates/workspace-services/inner-eye-deep-learning/#running-the-innereye-helloworld-on-aml-compute-cluster","text":"Log onto a VM in the workspace, open PowerShell and run: git clone https://github.com/microsoft/InnerEye-DeepLearning cd InnerEye-DeepLearning git lfs install git lfs pull conda init conda env create --file environment.yml Restart PowerShell and navigate to the \"InnerEye-DeepLearning\" folder conda activate InnerEye Open Azure Storage Explorer and connect to your Storage Account using name and access key On the storage account create a container with name datasets and a folder named hello_world Copy dataset.csv file from Tests/ML/test_data/dataset.csv to the hello_world folder Copy the whole train_and_test_data folder from Test/ML/test_data/train_and_test_data to the hello_world folder Update the following variables in InnerEye/settings.yml : subscription_id, resource_group, workspace_name, cluster (see AML setup for more details). Open your browser to ml.azure.com, login, select the right Subscription and AML workspace and then navigate to Data stores . Create a New datastore named innereyedatasets and link it to your storage account and datasets container. Back from PowerShell run python InnerEye/ML/runner.py --model=HelloWorld --azureml=True The runner will provide you with a link and ask you to open it to login. Copy the link and open it in browser (Edge) on the DSVM and login. The run will continue after login. In your browser navigate to ml.azure.com and open the Experiments tab to follow the progress of the training","title":"Running the InnerEye HelloWorld on AML Compute Cluster"},{"location":"workspace-templates/workspace-services/inner-eye-inference/","text":"InnerEye Inference service bundle See: https://github.com/microsoft/InnerEye-Inference Prerequisites A workspace with an InnerEye Deep Learning bundle installed Manual Deployment Create a service principal with contributor rights over the subscription. This will be replaced with a Managed Identity in the future: az ad sp create-for-rbac --name <sp-name> --role Contributor --scopes /subscriptions/<subscription-id> Create a copy of templates/workspace_services/innereye_inference/.env.sample with the name .env and update the variables with the appropriate values. Environment variable name Description WORKSPACE_ID The 4 character unique identifier used when deploying the base workspace bundle. AZUREML_WORKSPACE_NAME Name of the Azure ML workspace deployed as part of the Azure ML workspace service prerequisite. AZUREML_ACR_ID ID of the Azure Container Registry deployed as part of the Azure ML workspace service prerequisite. INFERENCE_SP_CLIENT_ID Service principal client ID used by the inference service to connect to Azure ML. Use the output from the step above. INFERENCE_SP_CLIENT_SECRET Service principal client secret used by the inference service to connect to Azure ML. Use the output from the step above. Build and deploy the InnerEye Inference service make porter-build DIR=./templates/workspace_services/innereye_inference make porter-install DIR=./templates/workspace_services/innereye_inference Log onto a VM in the workspace and run: git clone https://github.com/microsoft/InnerEye-Inference cd InnerEye-Inference az webapp up --name <inference-app-name> -g <resource-group-name> Configuring and testing inference service The workspace service provision an App Service Plan and an App Service for hosting the inference webapp. The webapp will be integrated into the workspace network, allowing the webapp to connect to the AML workspace. Following the setup you will need to: Create a new container in your storage account for storing inference images called inferencedatastore . Create a new folder in that container called imagedata . Navigate to the ml.azure.com, Datastores and create a new datastore named inferencedatastore and connect it to the newly created container. The key used for authentication is the inference_auth_key provided as an output of the service deployment. Test the service by sending a GET or POST command using curl or Invoke-WebRequest: Simple ping: Invoke-WebRequest https://yourservicename.azurewebsites.net/v1/ping -Headers @{'Accept' = 'application/json'; 'API_AUTH_SECRET' = 'your-secret-1234-1123445'} Test connection with AML: Invoke-WebRequest https://yourservicename.azurewebsites.net/v1/model/start/HelloWorld:1 -Method POST -Headers @{'Accept' = 'application/json'; 'API_AUTH_SECRET' = 'your-secret-1234-1123445'}","title":"InnerEye Inferencing"},{"location":"workspace-templates/workspace-services/inner-eye-inference/#innereye-inference-service-bundle","text":"See: https://github.com/microsoft/InnerEye-Inference","title":"InnerEye Inference service bundle"},{"location":"workspace-templates/workspace-services/inner-eye-inference/#prerequisites","text":"A workspace with an InnerEye Deep Learning bundle installed","title":"Prerequisites"},{"location":"workspace-templates/workspace-services/inner-eye-inference/#manual-deployment","text":"Create a service principal with contributor rights over the subscription. This will be replaced with a Managed Identity in the future: az ad sp create-for-rbac --name <sp-name> --role Contributor --scopes /subscriptions/<subscription-id> Create a copy of templates/workspace_services/innereye_inference/.env.sample with the name .env and update the variables with the appropriate values. Environment variable name Description WORKSPACE_ID The 4 character unique identifier used when deploying the base workspace bundle. AZUREML_WORKSPACE_NAME Name of the Azure ML workspace deployed as part of the Azure ML workspace service prerequisite. AZUREML_ACR_ID ID of the Azure Container Registry deployed as part of the Azure ML workspace service prerequisite. INFERENCE_SP_CLIENT_ID Service principal client ID used by the inference service to connect to Azure ML. Use the output from the step above. INFERENCE_SP_CLIENT_SECRET Service principal client secret used by the inference service to connect to Azure ML. Use the output from the step above. Build and deploy the InnerEye Inference service make porter-build DIR=./templates/workspace_services/innereye_inference make porter-install DIR=./templates/workspace_services/innereye_inference Log onto a VM in the workspace and run: git clone https://github.com/microsoft/InnerEye-Inference cd InnerEye-Inference az webapp up --name <inference-app-name> -g <resource-group-name>","title":"Manual Deployment"},{"location":"workspace-templates/workspace-services/inner-eye-inference/#configuring-and-testing-inference-service","text":"The workspace service provision an App Service Plan and an App Service for hosting the inference webapp. The webapp will be integrated into the workspace network, allowing the webapp to connect to the AML workspace. Following the setup you will need to: Create a new container in your storage account for storing inference images called inferencedatastore . Create a new folder in that container called imagedata . Navigate to the ml.azure.com, Datastores and create a new datastore named inferencedatastore and connect it to the newly created container. The key used for authentication is the inference_auth_key provided as an output of the service deployment. Test the service by sending a GET or POST command using curl or Invoke-WebRequest: Simple ping: Invoke-WebRequest https://yourservicename.azurewebsites.net/v1/ping -Headers @{'Accept' = 'application/json'; 'API_AUTH_SECRET' = 'your-secret-1234-1123445'} Test connection with AML: Invoke-WebRequest https://yourservicename.azurewebsites.net/v1/model/start/HelloWorld:1 -Method POST -Headers @{'Accept' = 'application/json'; 'API_AUTH_SECRET' = 'your-secret-1234-1123445'}","title":"Configuring and testing inference service"},{"location":"workspace-templates/workspaces/azure-ml-dev-test-labs/","text":"Azure ML and Dev Test Labs Workspace This deploys a TRE workspace with the following services: Azure ML Azure Dev Test Labs Please follow the above links to learn more about how to access the services and any firewall rules that they will open in the workspace. Manual deployment Publish the bundles required for this workspace: Base Workspace make porter-build DIR=./templates/workspaces/base make porter-publish DIR=./templates/workspaces/base Azure ML Service make porter-build DIR=./templates/workspace_services/azureml make porter-publish DIR=./templates/workspace_services/azureml DevTest Labs Service make porter-build DIR=./templates/workspace_services/devtestlabs make porter-publish DIR=./templates/workspace_services/devtestlabs Create a copy of workspaces/azureml_devtestlabs/.env.sample with the name .env and update the variables with the appropriate values. Environment variable name Description WORKSPACE_ID A 4 character unique identifier for the workspace for this TRE. WORKSPACE_ID can be found in the resource names of the workspace resources; for example, a WORKSPACE_ID of ab12 will result in a resource group name for workspace of rg-<tre-id>-ab12 . Allowed characters: Alphanumeric. ADDRESS_SPACE The address space for the workspace virtual network. For example 192.168.1.0/24 Build and install the workspace: make porter-build DIR=./templates/workspaces/azureml_devtestlabs make porter-publish DIR=./templates/workspaces/azureml_devtestlabs make porter-install DIR=./templates/workspaces/azureml_devtestlabs","title":"Azure ML DevTest Labs"},{"location":"workspace-templates/workspaces/azure-ml-dev-test-labs/#azure-ml-and-dev-test-labs-workspace","text":"This deploys a TRE workspace with the following services: Azure ML Azure Dev Test Labs Please follow the above links to learn more about how to access the services and any firewall rules that they will open in the workspace.","title":"Azure ML and Dev Test Labs Workspace"},{"location":"workspace-templates/workspaces/azure-ml-dev-test-labs/#manual-deployment","text":"Publish the bundles required for this workspace: Base Workspace make porter-build DIR=./templates/workspaces/base make porter-publish DIR=./templates/workspaces/base Azure ML Service make porter-build DIR=./templates/workspace_services/azureml make porter-publish DIR=./templates/workspace_services/azureml DevTest Labs Service make porter-build DIR=./templates/workspace_services/devtestlabs make porter-publish DIR=./templates/workspace_services/devtestlabs Create a copy of workspaces/azureml_devtestlabs/.env.sample with the name .env and update the variables with the appropriate values. Environment variable name Description WORKSPACE_ID A 4 character unique identifier for the workspace for this TRE. WORKSPACE_ID can be found in the resource names of the workspace resources; for example, a WORKSPACE_ID of ab12 will result in a resource group name for workspace of rg-<tre-id>-ab12 . Allowed characters: Alphanumeric. ADDRESS_SPACE The address space for the workspace virtual network. For example 192.168.1.0/24 Build and install the workspace: make porter-build DIR=./templates/workspaces/azureml_devtestlabs make porter-publish DIR=./templates/workspaces/azureml_devtestlabs make porter-install DIR=./templates/workspaces/azureml_devtestlabs","title":"Manual deployment"},{"location":"workspace-templates/workspaces/base/","text":"Azure TRE base workspace Prerequisites A TRE environment Manual Deployment Create a copy of /templates/workspaces/base/.env.sample with the name .env and update the variables with the appropriate values. Environment variable name Description WORKSPACE_ID A 4 ter unique identifier for the workspace for this TRE. WORKSPACE_ID can be found in the resource names of the workspace resources; for example, a WORKSPACE_ID of ab12 will result in a resource group name for workspace of rg-<tre-id>-ab12 . Allowed characters: Alphanumeric. ADDRESS_SPACE The address space for the workspace virtual network. For example 192.168.1.0/24 Build and deploy the base workspace make porter-build DIR=./templates/workspaces/base make porter-install DIR=./templates/workspaces/base","title":"Base"},{"location":"workspace-templates/workspaces/base/#azure-tre-base-workspace","text":"","title":"Azure TRE base workspace"},{"location":"workspace-templates/workspaces/base/#prerequisites","text":"A TRE environment","title":"Prerequisites"},{"location":"workspace-templates/workspaces/base/#manual-deployment","text":"Create a copy of /templates/workspaces/base/.env.sample with the name .env and update the variables with the appropriate values. Environment variable name Description WORKSPACE_ID A 4 ter unique identifier for the workspace for this TRE. WORKSPACE_ID can be found in the resource names of the workspace resources; for example, a WORKSPACE_ID of ab12 will result in a resource group name for workspace of rg-<tre-id>-ab12 . Allowed characters: Alphanumeric. ADDRESS_SPACE The address space for the workspace virtual network. For example 192.168.1.0/24 Build and deploy the base workspace make porter-build DIR=./templates/workspaces/base make porter-install DIR=./templates/workspaces/base","title":"Manual Deployment"},{"location":"workspace-templates/workspaces/inner-eye-deep-learning-inferencing/","text":"InnerEye Deep Learning and Inference Workspace This deploys a TRE workspace with the following services: Azure ML Azure Dev Test Labs InnerEye deep learning InnerEye Inference Follow the links to learn more about how to access the services and any firewall rules that they will open in the workspace. Manual deployment Publish the bundles required for this workspace: Base Workspace make porter-build DIR=./templates/workspaces/base make porter-publish DIR=./templates/workspaces/base Azure ML Service make porter-build DIR=./templates/workspace_services/azureml make porter-publish DIR=./templates/workspace_services/azureml DevTest Labs Service make porter-build DIR=./templates/workspace_services/devtestlabs make porter-publish DIR=./templates/workspace_services/devtestlabs InnerEye Deep Learning Service make porter-build DIR=./templates/workspace_services/innereye_deeplearning make porter-publish DIR=./templates/workspace_services/innereye_deeplearning InnerEye Inference Service make porter-build DIR=./templates/workspace_services/innereye_inference make porter-publish DIR=./templates/workspace_services/innereye_inference Create a service principal with contributor rights over Azure ML: az ad sp create-for-rbac --name <sp-name> --role Contributor --scopes /subscriptions/<subscription-id>/resourceGroups/<resource-group-name>/providers/Microsoft.MachineLearningServices/workspaces/<workspace-name> Create a copy of workspaces/innereye_deeplearning_inference/.env.sample with the name .env and update the variables with the appropriate values. Environment variable name Description WORKSPACE_ID A 4 ter unique identifier for the workspace for this TRE. WORKSPACE_ID can be found in the resource names of the workspace resources; for example, a WORKSPACE_ID of ab12 will result in a resource group name for workspace of rg-<tre-id>-ab12 . Allowed characters: Alphanumeric. ADDRESS_SPACE The address space for the workspace virtual network. For example 192.168.1.0/24 INFERENCE_SP_CLIENT_ID Service principal client ID used by the inference service to connect to Azure ML. Use the output from the step above. INFERENCE_SP_CLIENT_SECRET Service principal client secret used by the inference service to connect to Azure ML. Use the output from the step above. Build and install the workspace: make porter-publish DIR=./templates/workspaces/innereye_deeplearning_inference make porter-install DIR=./templates/workspaces/innereye_deeplearning_inference","title":"InnerEye Inferencing"},{"location":"workspace-templates/workspaces/inner-eye-deep-learning-inferencing/#innereye-deep-learning-and-inference-workspace","text":"This deploys a TRE workspace with the following services: Azure ML Azure Dev Test Labs InnerEye deep learning InnerEye Inference Follow the links to learn more about how to access the services and any firewall rules that they will open in the workspace.","title":"InnerEye Deep Learning and Inference Workspace"},{"location":"workspace-templates/workspaces/inner-eye-deep-learning-inferencing/#manual-deployment","text":"Publish the bundles required for this workspace: Base Workspace make porter-build DIR=./templates/workspaces/base make porter-publish DIR=./templates/workspaces/base Azure ML Service make porter-build DIR=./templates/workspace_services/azureml make porter-publish DIR=./templates/workspace_services/azureml DevTest Labs Service make porter-build DIR=./templates/workspace_services/devtestlabs make porter-publish DIR=./templates/workspace_services/devtestlabs InnerEye Deep Learning Service make porter-build DIR=./templates/workspace_services/innereye_deeplearning make porter-publish DIR=./templates/workspace_services/innereye_deeplearning InnerEye Inference Service make porter-build DIR=./templates/workspace_services/innereye_inference make porter-publish DIR=./templates/workspace_services/innereye_inference Create a service principal with contributor rights over Azure ML: az ad sp create-for-rbac --name <sp-name> --role Contributor --scopes /subscriptions/<subscription-id>/resourceGroups/<resource-group-name>/providers/Microsoft.MachineLearningServices/workspaces/<workspace-name> Create a copy of workspaces/innereye_deeplearning_inference/.env.sample with the name .env and update the variables with the appropriate values. Environment variable name Description WORKSPACE_ID A 4 ter unique identifier for the workspace for this TRE. WORKSPACE_ID can be found in the resource names of the workspace resources; for example, a WORKSPACE_ID of ab12 will result in a resource group name for workspace of rg-<tre-id>-ab12 . Allowed characters: Alphanumeric. ADDRESS_SPACE The address space for the workspace virtual network. For example 192.168.1.0/24 INFERENCE_SP_CLIENT_ID Service principal client ID used by the inference service to connect to Azure ML. Use the output from the step above. INFERENCE_SP_CLIENT_SECRET Service principal client secret used by the inference service to connect to Azure ML. Use the output from the step above. Build and install the workspace: make porter-publish DIR=./templates/workspaces/innereye_deeplearning_inference make porter-install DIR=./templates/workspaces/innereye_deeplearning_inference","title":"Manual deployment"},{"location":"workspace-templates/workspaces/inner-eye-deep-learning/","text":"InnerEye Deep Learning Workspace This deploys a TRE workspace with the following services: Azure ML Azure Dev Test Labs InnerEye deep learning Follow the links to learn more about how to access the services and any firewall rules that they will open in the workspace. Manual deployment Publish the bundles required for this workspace: Base Workspace make porter-build DIR=./templates/workspaces/base make porter-publish DIR=./templates/workspaces/base Azure ML Service make porter-build DIR=./templates/workspace_services/azureml make porter-publish DIR=./templates/workspace_services/azureml DevTest Labs Service make porter-build DIR=./templates/workspace_services/devtestlabs make porter-publish DIR=./templates/workspace_services/devtestlabs InnerEye Deep Learning Service make porter-build DIR=./templates/workspace_services/innereye_deeplearning make porter-publish DIR=./templates/workspace_services/innereye_deeplearning Create a copy of workspaces/innereye_deeplearning/.env.sample with the name .env and update the variables with the appropriate values. Environment variable name Description WORKSPACE_ID A 4 character unique identifier for the workspace for this TRE. WORKSPACE_ID can be found in the resource names of the workspace resources; for example, a WORKSPACE_ID of ab12 will result in a resource group name for workspace of rg-<tre-id>-ab12 . Allowed characters: Alphanumeric. ADDRESS_SPACE The address space for the workspace virtual network. For example 192.168.1.0/24 Build and install the workspace: make porter-build DIR=./templates/workspaces/innereye_deeplearning make porter-publish DIR=./templates/workspaces/innereye_deeplearning make porter-install DIR=./templates/workspaces/innereye_deeplearning","title":"InnerEye Deep Learning"},{"location":"workspace-templates/workspaces/inner-eye-deep-learning/#innereye-deep-learning-workspace","text":"This deploys a TRE workspace with the following services: Azure ML Azure Dev Test Labs InnerEye deep learning Follow the links to learn more about how to access the services and any firewall rules that they will open in the workspace.","title":"InnerEye Deep Learning Workspace"},{"location":"workspace-templates/workspaces/inner-eye-deep-learning/#manual-deployment","text":"Publish the bundles required for this workspace: Base Workspace make porter-build DIR=./templates/workspaces/base make porter-publish DIR=./templates/workspaces/base Azure ML Service make porter-build DIR=./templates/workspace_services/azureml make porter-publish DIR=./templates/workspace_services/azureml DevTest Labs Service make porter-build DIR=./templates/workspace_services/devtestlabs make porter-publish DIR=./templates/workspace_services/devtestlabs InnerEye Deep Learning Service make porter-build DIR=./templates/workspace_services/innereye_deeplearning make porter-publish DIR=./templates/workspace_services/innereye_deeplearning Create a copy of workspaces/innereye_deeplearning/.env.sample with the name .env and update the variables with the appropriate values. Environment variable name Description WORKSPACE_ID A 4 character unique identifier for the workspace for this TRE. WORKSPACE_ID can be found in the resource names of the workspace resources; for example, a WORKSPACE_ID of ab12 will result in a resource group name for workspace of rg-<tre-id>-ab12 . Allowed characters: Alphanumeric. ADDRESS_SPACE The address space for the workspace virtual network. For example 192.168.1.0/24 Build and install the workspace: make porter-build DIR=./templates/workspaces/innereye_deeplearning make porter-publish DIR=./templates/workspaces/innereye_deeplearning make porter-install DIR=./templates/workspaces/innereye_deeplearning","title":"Manual deployment"}]}